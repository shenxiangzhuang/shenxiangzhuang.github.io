---
title: 'Perplexity'
description: 'Perplexity 的前世今生'
date: 2026-02-08
tags: ['AI', 'NLP']
authors: ['mathew']
draft: false
---

## 0. 动机：为什么需要 PPL

在语言建模里，模型的核心产出是概率分布：它告诉我们“下一词有多大可能”。
但这带来一个现实问题：**如何用一个标量去衡量模型整体的预测能力**？

直接比较整句的概率 $Q(W)$ 不公平，因为句子越长，概率必然越小。
我们需要一个长度无关、可比较、且与训练目标一致的度量。
这就是困惑度（Perplexity, PPL）存在的理由。

为了得到它，我们必须先回到信息论的三块基石：熵、交叉熵与困惑度的指数形式。

## 1. 定义：从信息论出发

为了把上面的动机落到数学上，我们先建立三个基础概念：熵、交叉熵与困惑度。

### 1.1 熵 (Entropy)：不确定性的尺度

熵是衡量一个概率分布 $P$ 内部“不确定性”的指标。

假设有一个离散随机变量 $X$，其可能取值为 $x_1, ..., x_n$，服从真实的概率分布 $P(x)$。香农（Shannon）定义的熵 $H(P)$ 为：

$$
H(P) = -\sum_{x} P(x) \log_2 P(x)
$$

*注：通常使用以 2 为底的对数，单位为比特（bits）。如果使用自然对数 $e$，单位为纳特（nats）。*

**关键性质与证明：**

1.  **极小值（确定性）：** 当且仅当分布集中在某一个结果上时（即某个 $P(x_k)=1$，其余为 0），熵取最小值 0。
*   **证明：** $H(P) = -1 \cdot \log_2 1 - \sum 0 \cdot \log_2 0 = 0$。（根据极限 $\lim_{x\to 0+} x \log x = 0$）。这对应了完全确定的情况，没有不确定性。

1.  **极大值（均匀性）：** 当且仅当分布为均匀分布时（即所有 $P(x_i) = 1/n$），熵取最大值 $\log_2 n$。
*   **证明（利用 Jensen 不等式）：**
    由于对数函数 $y=\log x$ 是凹函数，根据 Jensen 不等式 $E[\log X] \le \log E[X]$：
    $$
    H(P) = \sum_{i} P(x_i) \log \frac{1}{P(x_i)} \le \log \left( \sum_{i} P(x_i) \frac{1}{P(x_i)} \right) = \log \left( \sum_{i} 1 \right) = \log n
    $$
    当且仅当随机变量 $1/P(x_i)$ 为常数（即所有 $P(x_i)$ 相等）时取等号。此时不确定性最大。

### 1.2 交叉熵 (Cross-Entropy)：拟合的代价

如果说熵是上帝视角下的最优编码长度，那么交叉熵就是凡人视角的现实难度。

在现实世界中，我们往往无法得知真实的分布 $P$，只能构建一个模型分布 $Q$ 去近似它。当我们使用基于 $Q$ 优化的编码方案（比如哈夫曼编码），来处理实际上服从 $P$ 的数据时，必然会产生效率损失（即具体的编码长度会比理论最优值更长）。

**定义：**
交叉熵 $H(P, Q)$ 衡量的是：在真实分布为 $P$ 的情况下，使用模型 $Q$ 进行预测（或编码）所产生的平均惊诧度（或平均比特数）。

$$
H(P, Q) = \mathbb{E}_{x \sim P} [-\log Q(x)] = -\sum_{x} P(x) \log Q(x)
$$

**分解定理（核心直觉）：**
为什么交叉熵总是大于熵？我们可以通过简单的代数变换将其拆解为两部分：

$$
\begin{aligned}
H(P, Q) &= -\sum P(x) \log Q(x) \\
&= -\sum P(x) \log \left( P(x) \cdot \frac{Q(x)}{P(x)} \right) \\
&= \underbrace{-\sum P(x) \log P(x)}_{H(P)} + \underbrace{\sum P(x) \log \frac{P(x)}{Q(x)}}_{D_{KL}(P || Q)}
\end{aligned}
$$

即：

$$
H(P, Q) = H(P) + D_{KL}(P || Q)
$$

这个等式揭示了交叉熵的物理意义：
1.  **$H(P)$（内在不确定性）**：这是数据本身固有的熵，无论模型多强大都无法消除的底噪（Aleatoric Uncertainty）。
2.  **$D_{KL}(P || Q)$（KL 散度）**：这是因为模型 $Q$ 对真实分布 $P$ 的认知偏差而产生的**额外代价**。

**吉布斯不等式及其证明：**
我们直觉上也知道“错误的模型肯定比正确的模型差”，即 $H(P, Q) \ge H(P)$。这等价于证明 $D_{KL}(P || Q) \ge 0$。

*   **证明（利用 Jensen 不等式）：**
    考虑 KL 散度的相反数（这里取负号以便使用 $\log$ 凹函数的性质）：
    $$
    -D_{KL}(P || Q) = \sum_{x} P(x) \log \frac{Q(x)}{P(x)}
    $$
    根据 Jensen 不等式 $E[\log X] \le \log E[X]$：
    $$
    \sum_{x} P(x) \log \frac{Q(x)}{P(x)} \le \log \left( \sum_{x} P(x) \frac{Q(x)}{P(x)} \right) = \log \left( \sum_{x} Q(x) \right) = \log 1 = 0
    $$
    
    因为 $-D_{KL} \le 0$，所以 $D_{KL} \ge 0$。
    
    **结论：** 当且仅当 $Q(x) = P(x)$ 时，KL 散度为 0，交叉熵取得最小值 $H(P)$。这就是机器学习中要把 Minimize Cross Entropy 作为目标函数的原因——对于固定的数据集（$P$ 固定），最小化交叉熵等价于最小化 KL 散度，即让模型 $Q$ 逼近真相 $P$。

### 1.3 困惑度 (Perplexity)：熵的指数化

**Perplexity 这个词是香农发明的吗？**
并不是。但是 **$2^H$** 这个数学概念所蕴含的直觉，最早可以追溯到香农。

在 1948 年的奠基之作 *A Mathematical Theory of Communication* 中，香农定义了熵。虽然他主要使用“比特”（Bits）作为单位，但也阐述了 $H = \log_2 N$ 的关系：如果一个信源包含 $N$ 个**等概率**的符号，其熵为 $\log_2 N$。

反过来说，对于任意一个熵为 $H$ 的概率分布，我们都可以找到一个“等效”的均匀分布，其包含的选项数量为 $2^H$。

**Motivation（动机）：**
为什么要关注 $2^H$？因为它将抽象的“比特”还原为了线性的“可能性数量”。

- 比如一个信源的熵是 5 bits。这句话对普通人很抽象。
- 但如果计算 $2^5 = 32$，物理意义瞬间清晰：这个信源的不确定性，**等价于**从 32 个等可能的符号中随机选择一个。

这就是 Perplexity 的本质——它是**熵的指数形式**：

$$
PP = 2^{H(P, Q)}
$$

或者如果交叉熵是基于自然对数（$e$）计算的：

$$
PP = e^{H(P, Q)}
$$

所谓 **"Perplexity"** 这个名字，则是直到 1970 年代，才由 Fred Jelinek 等人在语音识别领域正式命名的。


## 2. 引入 NLP：从分布到序列

在第一部分，我们讨论的是“单个随机变量”的不确定性。
而 NLP 的对象是**序列**，我们需要把“一个句子”当成一个随机事件来建模。

### 2.1 定义样本空间与分布

为了复用第一章的数学工具，我们需要明确 $P$ 和 $Q$ 到底是在什么东西上的分布。关键在于视角的转换：**不要把单个“词”看作样本，要把“整句话”看作一个独立的随机事件。**

*   **样本空间 (Support)**：
    假设我们要处理长度为 $N$ 的序列。我们的随机变量 $X$ 就是**整个序列**。
    因此，样本空间 $\Omega$ 就是由**所有可能的长度为 $N$ 的词序列**构成的集合。
    *   **玩具例子**：设词表 $V=\{A, B\}$，长度 $N=2$。那么样本空间就是
        $$
        \Omega = \{AA, AB, BA, BB\}
        $$
        测试语料只是其中一种组合，比如 $W=AB$。

*   **模型分布 $Q(W)$ (联合概率)**：
    模型 $Q$ 本质上是一个**定义在序列空间 $\Omega$ 上的联合分布**，目标是给出整句话 $W$ 出现的概率 $Q(W)$。
    直接对从 $\Omega$ 中采样出的整句建模是非常困难的（维度爆炸）。因此，在 NLP 中我们利用**概率链式法则 (Chain Rule)** 将同一个联合分布“按前缀条件化”拆解为一串条件概率：
    $$
    Q(W) = Q(w_1) \cdot Q(w_2 | w_1) \cdot ... \cdot Q(w_N | w_{<N}) = \prod_{i=1}^N Q(w_i | w_{<i})
    $$

    **玩具例子对应**：
    对 $W=AB$，有
    $$
    Q(AB) = Q(A) \cdot Q(B|A)
    $$

    形式上，这些条件概率都来自同一个联合分布：
    $$
    Q(w_i | w_{<i}) = \frac{Q(w_1, ..., w_i)}{Q(w_1, ..., w_{i-1})}
    $$
    这里的 $Q(w_1, ..., w_i)$ 不是“改了输入长度”，而是**对同一联合分布的边缘化**。更直观的写法是：
    $$
    Q(w_{\le i}) = \sum_{w_{i+1} \in V} \sum_{w_{i+2} \in V} \cdots \sum_{w_N \in V} Q(w_1, w_2, ..., w_N)
    $$

    **玩具例子对应**：
    $$
    Q(A) = Q(AA) + Q(AB)
    $$
    因此
    $$
    Q(B|A) = \frac{Q(AB)}{Q(A)} = \frac{Q(AB)}{Q(AA) + Q(AB)}
    $$

    也就是说，把“后续所有可能的词”都加总起来，就得到“前缀发生”的概率。
    因此，$Q$ 始终是定义在“整句”上的分布；$Q(w_i | w_{<i})$ 只是把同一个 $Q$ 对后续位置求和后，再按前缀条件化得到的视角。

    **玩具例子小结**：
    对 $V=\{A,B\}, N=2$ 来说，联合分布 $Q(AA), Q(AB), Q(BA), Q(BB)$ 就是模型的“全貌”。
    从这四个数可以得到前缀边缘 $Q(A)=Q(AA)+Q(AB)$，再得到条件概率 $Q(B|A)$，最后通过链式法则还原回 $Q(AB)$。

*   **真实分布 $P(W)$**：
    这是自然语言的客观真理。实际中我们无法直接获得它。
    但在**评估阶段**，我们拥有一组真实语料（Test Corpus）。

    常见做法是用**经验分布 (Empirical Distribution)** 近似真实分布。为了突出推导的主干，我们先用单个样本 $W_{obs}$ 说明：
    $$
    P(W) = \begin{cases} 1 & \text{if } W = W_{obs} \\ 0 & \text{other} \end{cases}
    $$
    在真实评估中，我们对整个测试集取平均，形式上只是在求和里加入更多样本而已，推导不变。

### 2.2 推导序列的交叉熵

回到第一章的交叉熵定义：
$$
H(P, Q) = -\sum_{W \in \Omega} P(W) \log Q(W)
$$

将我们定义的序列分布代入：
由于 $P(W)$ 仅在测试集 $W_{obs}$ 处为 1，其余为 0，求和符合瞬间简化为一项：
$$
H(P, Q) = -1 \cdot \log Q(W_{obs}) = -\log Q(W_{obs})
$$

这就是整个序列的总惊诧度。

### 2.3 从总熵到“每个词的平均熵”

但是，总惊诧度与序列长度 $N$ 正相关。如果不做归一化，长句子的 Loss 永远比短句子大。
为了衡量模型在**“单个词”**层面的平均预测能力，我们需要计算**每个 Token 的平均交叉熵**（Average Cross Entropy per Token）：

$$
H_{\text{avg}}(P, Q) = \frac{1}{N} H(P, Q) = -\frac{1}{N} \log Q(W_{obs})
$$

利用 $\log$ 的性质展开 $Q(W_{obs})$：
$$
H_{\text{avg}}(P, Q) = -\frac{1}{N} \sum_{i=1}^{N} \log Q(w_i | w_{<i})
$$

这正是我们在训练代码中经常看到的 `CrossEntropyLoss`。

### 2.4 定义 NLP 中的 PPL

现在，逻辑闭环了。
既然 $PP = e^{H(P,Q)}$，那么在 NLP 中，我们同样使用平均熵的指数形式：

$$
\text{PPL} = e^{H_{\text{avg}}(P, Q)} = e^{-\frac{1}{N} \sum_{i=1}^{N} \log Q(w_i | w_{<i})}
$$

这个推导过程清楚地表明：**Perplexity 就是测试集在模型下的“平均负对数似然”的指数形式。**

**直观理解 (Wikipedia 视角)：**
如 [Wikipedia](https://en.wikipedia.org/wiki/Perplexity) 所述，利用对数性质，该公式等价于模型对正确词预测概率倒数的几何平均：
$$
\text{PPL} = \sqrt[N]{\prod_{i=1}^{N} \frac{1}{Q(w_i | w_{<i})}}
$$

这正是我们在训练时优化的 **Cross Entropy Loss**（即每一个 Token 损失的算术平均值）。

因此，我们在训练日志中看到的 Loss 与 PPL 存在着最简单的指数关系：

$$
\text{PPL} = e^{\text{Loss}}
$$

*(注：这里的 Loss 需基于自然对数 $e$ 计算；如果是以 2 为底，则是 $2^{\text{Loss}}$)*

### 2.5 历史：Jelinek 的贡献

Perplexity 并不是一开始就生长在 NLP 里的。它是在 1970 年代末被引入语音识别（Automatic Speech Recognition, ASR）领域的。這一概念最早可以追溯到 IBM T.J. Watson 研究中心的经典工作：

> **Paper:** [Jelinek, F., Mercer, R. L., Bahl, L. R., & Baker, J. K. (1977). *Perplexity—a measure of the difficulty of speech recognition tasks.*](https://asa.scitation.org/doi/abs/10.1121/1.2016299)

**核心动机：解耦。**
当时，研究人员主要关注词错率（WER）。但这不仅取决于**语言模型**（预测下一个词的能力），还取决于**声学模型**（听清声音的能力）。Fred Jelinek 等人提出 PPL，旨在衡量**语言任务本身的难度**，从而能独立地优化和评估语言模型。

## 3. 物理意义：推导与直觉

从上面的数学公式中，我们如何获得直观的物理感受？

### 3.1 统计学直觉：分支系数（Branching Factor）

想象一个骰子。
- 如果是完美均匀的 6 面骰子，每个面概率是 $1/6$。
    - 熵是 $\log_2 6$。
    - 困惑度是 $2^{\log_2 6} = 6$。
    - **物理意义：** 你对下一个结果的困惑程度，相当于在 **6 个选项中盲选**。

- 如果是作弊骰子，如果你 100% 确定它会出 6 点。
    - 熵是 0。
    - 困惑度是 $2^0 = 1$。
    - **物理意义：** 你的困惑程度为 1（完全不困惑，只有 1 个选项）。

所以，Perplexity 本质上是在衡量**“加权平均的分支因子”**。

### 3.2 NLP 中的物理意义

在语言模型中，PPL 表示**“模型在每一步预测时，平均以为有多少个合理的候选词”**。

- **PPL = 100**：模型在预测下一个词时，就像是在 100 个词里随机瞎猜一样迷茫。
- **PPL = 1**：模型完全确定下一个词是什么。

**举例**：
句子："The cat sat on the [mat]"。
- **模型 A** 预测 "mat" 的概率是 0.1。$PPL \approx 1/0.1 = 10$。
- **模型 B** 预测 "mat" 的概率是 0.5。$PPL \approx 1/0.5 = 2$。

模型 B 的 PPL 更低，说明它更确定，也就是更好。




## 4. 深入探讨：PPL 的局限性与误区

虽然 Perplexity 是 NLP 领域的“北极星指标”，但它绝非完美。学术界有大量研究表明，盲目追求低 PPL 往往是个陷阱，甚至无法反映模型在真实任务中的表现。

### 4.1 低 PPL $\neq$ 高质量生成 (The Likelihood Trap)

我们直觉上认为：PPL 越低，模型越强，生成的文本就越好。
**事实并非总是如此。**

ICLR 2020 的经典论文 *[The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)* (Holtzman et al.) 指出了一个反直觉的现象：
如果你强制模型总是选择概率最高（即 PPL 最低）的词（例如使用 Beam Search），生成的文本往往会陷入**死循环（Repetition Loops）**，内容变得极其枯燥且缺乏信息量。

**人类语言的特征：**
研究发现，人类说出的句子并不是每一步都概率最大的。人类语言的概率分布在“高概率”和“低概率”之间波动。好的生成策略（如 Nucleus Sampling, Top-p）不仅要考虑低困惑度，还要保留一定的随机性（温度），这样才能产生生动、有创造力的文本。

### 4.2 跨模型比较的“巨坑”：Tokenizer 差异

这是新手最容易犯的错误：拿着模型 A 的 PPL 和模型 B 的 PPL 直接比大小。

**记住结论：只有当两个模型使用完全相同的 Tokenizer（分词器）和词表时，PPL 的比较才有意义。**

原理很简单：PPL 是对**每个 Token** 的平均惊诧度。

$$
PPL \propto \frac{1}{\text{Token 数量}}
$$

- **字级别模型 (Character-level)**：把每个字母当成一个 Token。序列 $N$ 变得非常大（一个单词变成 5-6 个 Token）。因为预测下一个字母通常比预测下一个词容易，且分母 $N$ 很大，计算出的 PPL 数值通常很低。
- **词级别模型 (Word-level)**：把整个单词当成一个 Token。序列 $N$ 小，PPL 数值通常很高。

如果你比较一个 BPE Tokenizer（平均 1 个词分 1.3 个 Token）和一个 Word Tokenizer，如果不进行归一化（比如统一换算成 Bits-Per-Character），这种比较就是毫无意义的“关公战秦琼”。

### 4.3 无法衡量事实与推理

PPL 衡量的是**“拟合数据的能力”**，而不是**“解决问题的能力”**。

- 一个只会背诵维基百科的模型，PPL 可以刷得非常低。
- 但如果你问它：“如果此时抛出一个红色的球，它会掉下来吗？”
    - 模型 A 理解了物理规律，回答“会”。
    - 模型 B 只是记住了语料中“抛出...掉下”的共现概率高，也回答“会”。

从 PPL 上看，两者都很优秀。但一旦遇到语料中未出现过的复杂推理题（OOD, Out-Of-Distribution），死记硬背的模型（PPL 低）可能会惨败，而具备泛化能力的模型才是有用的。
因此，现代大模型评估越来越依赖 **MMLU、GS8K** 等具体任务的评测集，PPL 更多退居为预训练阶段监控收敛情况的指标，而非最终能力的裁判。

## 5. 总结

Perplexity 是一个兼具数学优雅与物理直觉的指标。
- 从**信息论**看，它是 $2$ 的熵次幂，代表不确定性。
- 从**统计学**看，它是预测下一个词时的加权平均分支系数。
- 从**工程**看，它是 $e^{\text{CrossEntropyLoss}}$，是模型训练的直接目标。

理解 PPL，不仅是为了看懂论文中的数字，更是为了理解语言模型的核心目标：**在不确定性的海洋中，寻找确定性的航线。**
