---
title: '信息论中的Perplexity'
description: '从赛马赌局推导出的不确定性度量'
date: 2026-02-08
tags: ['AI', 'NLP']
authors: ['mathew']
draft: false
order: 1
---


我们通常通过公式死记硬背 Perplexity（困惑度），但如果回到信息论的源头，这一切其实源于一个非常直观的问题：**我们要花多少口舌，才能把一件事情描述清楚？**

## 直觉先行：赛马场的通讯课

让我们从信息论经典教材 Cover & Thomas 中的一个 赌马（Horse Race） 例子开始。

假设你正在观看一场由 8 匹马参加的比赛，你需要给远在城里的博彩公司发消息，告诉他们哪匹马赢了。为了节省昂贵的电报费，你需要设计一套最省字符的编码方案。

### 场景一：势均力敌（盲选）

如果这 8 匹马实力相当，每一匹马获胜的概率都是 1/8。
为了区分这 8 个等可能的选项，你别无选择，必须使用定长的二进制编码：

| 编号 | 马 1 | 马 2 | 马 3 | 马 4 | 马 5 | 马 6 | 马 7 | 马 8 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 编码 | `000` | `001` | `010` | `011` | `100` | `101` | `110` | `111` |

**平均长度**：无论谁赢，你都必须发送 **3 个比特**。
$$ \text{Length} = \log_2 8 = 3 \text{ bits} $$

### 场景二：强弱分明（策略）

但现实往往不是均匀的。假设你通过内幕消息得知，马匹的强弱极其悬殊，真实的获胜概率分布 $P(x)$ 如下（引自 Jurafsky & Martin 经典教材案例）：

| 编号 | 马 1 | 马 2 | 马 3 | 马 4 | 马 5 | 马 6 | 马 7 | 马 8 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 胜率 $P(x)$ | **1/2** | **1/4** | **1/8** | **1/16** | **1/64** | **1/64** | **1/64** | **1/64** |

这时候如果你对于所有马都使用 `000`-`111` 的 3 位编码就太浪费了。因为虽然你每次都发 3 个字，但绝大多数时候你发的都是"马 1"赢了的消息。

**直觉告诉我们**：既然“Horse 1”有一半的时间都在赢，为什么不给它分配一个极短的代码（比如就 1 位）？至于“Horse 8”，几百年才赢一次，代码长一点（比如 6 位）也无伤大雅，反正很少用到。

基于这个**“以此易彼”**的策略，我们可以凑出一种极其精妙的编码方案：

| 编号 | 马 1 | 马 2 | 马 3 | 马 4 | 马 5 | 马 6 | 马 7 | 马 8 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 概率 $P(x)$ | 1/2 | 1/4 | 1/8 | 1/16 | 1/64 | 1/64 | 1/64 | 1/64 |
| 巧妙的编码 | `0` | `10` | `110` | `1110` | `111100` | `111101` | `111110` | `111111` |
| 实际长度 $l_i$ | 1 | 2 | 3 | 4 | 6 | 6 | 6 | 6 |

**现在的平均长度是多少？** 让我们把每一项都加起来：

$$
\begin{aligned}
\text{Avg Length} &= \sum_{i=1}^{8} P(x_i) \cdot l_i \\
&= 1/2 \times 1 + 1/4 \times 2 + 1/8 \times 3 + 1/16 \times 4 + 4 \times (1/64 \times 6) \\
&= 0.5 + 0.5 + 0.375 + 0.25 + 0.375 \\
&= \mathbf{2 \text{ bits}}
\end{aligned}
$$

**惊人的结论**：虽然同样是 8 匹马，但在这种分布下，通过给高频事件分配短码，我们平均只需要 **2 个比特** 就能把结果传输出去。比之前的 3 个比特节省了 33%！

*敏锐的读者可能会问：Horse 3 为什么是 110 而不是 11？因为如果用了 11，那么所有 11 开头的编码（如 1110）都无法区分了。这就是“前缀码”的约束，我们必须给更小概率的事件留出编码空间。 这部分的细节在附录里有更深入的解释。*

---

### 从直觉到定义：为什么偏偏是 $-\log P(x)$？

回头审视上面那个“凑出来”的编码表，你会发现一个极其微妙的规律：
- Horse 1 概率是 $1/2$，我们给它的长度是 1。
- Horse 2 概率是 $1/4$，我们给它的长度是 2。
- Horse 5 概率是 $1/64$，我们给它的长度是 6。

似乎**最佳编码长度 $l$** 和 **概率 $P$** 之间存在着某种必然的数学映射：
$$ l = -\log_2 P $$

这是巧合吗？不。我们可以用**数学优化**的方法严格证明：这是在有限的编码资源下，使得平均长度最短的唯一解。

我们要解决的是一个**带约束的求极值问题**（记 $P_i = P(x_i)$）：

1.  **目标（Goal）**：最小化平均码长
    $$ \min \sum_{i} P_i \cdot l_i $$
2.  **约束（Constraint）**：前缀码必须满足 Kraft 不等式
    $$ \sum_{i} 2^{-l_i} \le 1 $$
    *具体证明见本文附录。*

为简洁起见，我们先把整数码长放松到实数域，并把 Kraft 不等式取等号。理由是：若最优解处有“剩余份额”，则可以整体减小所有 $l_i$ 使平均码长进一步下降，矛盾。

首先，构造拉格朗日函数:

$$
\mathcal{L}(l_1,\dots,l_n,\lambda) = \sum_i P_i \, l_i + \lambda\left(\sum_i 2^{-l_i} - 1\right)
$$

然后，对 $l_i$ 求导并令其为零：

$$
\frac{\partial \mathcal{L}}{\partial l_i} = P_i - \lambda \ln 2 \cdot 2^{-l_i} = 0
$$
$$
2^{-l_i} = c P_i, \quad c = \frac{1}{\lambda \ln 2}
$$

最后，用归一化确定常数：

$$
\sum_i 2^{-l_i} = 1 \Rightarrow c \sum_i P_i = 1 \Rightarrow c = 1
$$

因此 $2^{-l_i} = P_i$，两边取 $\log_2$ 得最优码长：

$$
\boxed{l_i = -\log_2 P_i}
$$



**这就是信息论的基石：** 一个事件越不可能发生（$P$ 越小），它带来的意外程度就越大，我们需要越长的编码来描述它。这个核心量 $-\log P(x)$ 在学术界有许多别名，它们从不同侧面描述了同一个物理事实：

*   **自信息 (Self-information)**：该事件自带的固有信息价值。
*   **惊诧度 (Surprisal)**：当这件事发生时，观察者感到的“震惊程度”。
*   **信息含量 (Information Content)**：为了消除该事件的不确定性，我们需要获取的信息量。

---

## 熵 (Entropy)：不确定性的标尺

现在我们可以自然地引出**熵**的定义了:

> The entropy of a random variable is 
> a lower bound on the average number of bits required to represent the
> random variable and also on the average number of questions needed to
> identify the variable in a game of “20 questions.”
> 
> — Cover & Thomas, *Elements of Information Theory*


简言之，**熵是描述随机变量所需的“最小平均编码长度”。**


对于分布 $P(x)$，既然每个事件 $x$ 的理想编码长度是 $-\log P(x)$，那么总体的平均长度就是：

$$
H(P) = \mathbb{E}_{x \sim P} [-\log P(x)] = -\sum_{x} P(x) \log_2 P(x)
$$

回到赛马的例子：
- **场景一（均匀）**：$H(P) = 3 \text{ bits}$。不确定性最大，编码长度最长。
- **场景二（偏斜）**：$H(P) = 2 \text{ bits}$。因为有了确定性偏向，不确定性降低，编码变短。

由此可见，熵是对**信息本身复杂程度**的度量。


## 交叉熵 (Cross-Entropy)：认知的代价

在现实中，我们往往不知道真实的马匹胜率 $P(x)$。博彩公司会根据经验估计出一个概率分布 $Q(x)$（即**模型分布**）。

当我们制定编码策略时，我们只能基于**我们的认知 $Q$** 来设定长度：我们认为 $x$ 发生的概率是 $Q(x)$，所以我们给它分配长度为 $-\log Q(x)$ 的代码。

但是，**现实是残酷的**。实际比赛的结果是按照**真实分布 $P(x)$** 发生的。

于是，实际的平均传输长度就变成了：

$$
H(P, Q) = \sum_{x} \underbrace{P(x)}_{\text{Reality}} \cdot \underbrace{\left( -\log_2 Q(x) \right)}_{\text{Our Strategy}}
$$

这就是 **交叉熵**。

如果我们猜对了（$Q=P$），那么交叉熵等于熵（$H(P,Q) = H(P)$），我们达到了理论最优。
如果我们猜错了（比如还是那个偏斜的赛马局，但我们误判为所有马胜率相等），我们就会给“马1”分配 3 bit (000)，而不是 1 bit (0)。

这中间的差值，就是 **KL 散度（Relative Entropy）**，即**无知/误判带来的额外代价**：

$$
H(P, Q) = H(P) + D_{KL}(P || Q)
$$

## 困惑度 (Perplexity)：从比特回归选项

在这个漫长的旅程最后，我们终于来到了 **Perplexity (PPL)**。

作为人类，我们对“比特”其实很不敏感。
- “这个模型的熵是 2 bit。” —— 听起来没什么感觉。
- “这个模型的熵是 5 bit。” —— 比上面那个大，但大多少？

Fred Jelinek 等人提出的 PPL，旨在把抽象的“比特数”还原回直观的“**选项数**”:

$$
\text{PPL} = 2^{H(P, Q)} \quad (\text{或者 } e^{H(P, Q)})
$$

那么，PPL = 4 意味着什么？让我们回到赛马的例子。
- **场景一**：熵 = 3 bits。PPL = $2^3 = 8$。
    - **物理意义**：这相当于在 8 匹势均力敌的马里猜，难度是 **8 选 1**。
- **场景二**：熵 = 2 bits。PPL = $2^2 = 4$。
    - **物理意义**：虽然场上还是 8 匹马，但由于强弱极其分明，你的预测难度大大降低了。这就**“等效于”**在一场只有 **4 匹** 势均力敌的马的比赛中猜赢家。

这就是 **“有效分支系数”（Effective Branching Factor）** 的概念。

PPL 告诉我们：**在模型的眼里，下一个词有多少个合理的选择。**
- 如果 PPL = 1000：模型面对 1000 个可能的词不知所措。
- 如果 PPL = 10：模型觉得只有 10 个词靠谱，既然范围缩小了，预测准确率自然就高了。

这就是为什么我们不仅追求低 Loss（数学上的优化目标），更关注低 PPL（物理上的直观指标）。

---

## 附录

### 为什么 Horse 3 不能用 `11`?

在正文中我们提到，Horse 3 的编码是 `110`（长度 3），而不是更短的 `11`（长度 2）。这背后的原因可以从两个互为表里的角度来解释：**解码的歧义性**和**空间的排他性**。

1. 拒绝歧义：前缀性质 (Prefix Property)

前缀码要求**任何一个字符的编码，都不能是另一个字符编码的前缀。** 只有这样，接收方才能在收到比特流时瞬间解码（Instant Decodability）。

*   **反面教材**：假设 Horse 3 是 `11`，Horse 4 是 `1110`。
    当接收方收到比特流 `1110...` 时，读到前两位 `11`，由于 `11` 既可能是 Horse 3 的全部，也可能是 Horse 4 的开头，接收方无法立刻判断，必须“往后看”。这违背了即时解码的原则。

2. 拒绝“德不配位”：空间份额守恒

更本质的解释依然是**“分蛋糕”**。编码空间总量是有限的（总和为 1）。

*   Horse 1 (`0`) 占用了 **1/2** 的空间。
*   Horse 2 (`10`) 占用了 **1/4** 的空间。
*   **如果 Horse 3 强行占用 `11`**：
    *   它是长度为 2 的编码，会消耗 **1/4** 的编码空间（$2^{-2} = 1/4$）。
    *   但这比它实际拥有的 **1/8** 概率要大，相当于它**“德不配位”**，占用了过多的资源。
    *   结果是：$1/2 + 1/4 + 1/4 = 1$。整个编码空间被填满了！Horse 4, 5, 6, 7, 8 将**无路可走**。

为了给后面的“穷兄弟”留条活路，Horse 3 只能克制一点，占用 `110`（空间 $1/8$），从而把 `111` 这个前缀（空间 $1/8$）剩下来，留给 Horse 4 及其后续的马去瓜分。

这就是为什么**概率**和**长度**必须严格对应（$2^{-l} \le P$）的物理原因：**你不仅代表你自己，你还要为比你概率更小的事件腾出空间。**

### Kraft 不等式的数学证明（二叉树模型）
我们可以用一颗**二叉树**来描述所有可能的二级制编码：
- 根节点代表空。
- 向左走代表 `0`，向右走代表 `1`。
- 每一个合法的码字，都是这就棵树上的一个**叶子节点**（或者被标记为已占用的终止节点）。

**前缀性质**意味着：如果一个节点被选为码字，那么它的所有子树（子孙节点）都不能再被使用。

证明思路如下：
假设我们只允许最长编码长度为 $L_{max}$。我们可以构建一颗深度为 $L_{max}$ 的满二叉树。
我们可以把每个码字 $l_i$ 都投影到最底层的第 $L_{max}$ 层去计算“投影面积”：
1.  第 $L_{max}$ 层总共有 $2^{L_{max}}$ 个叶子节点。
2.  对于任意一个长度为 $l_i$ 的码字，它实际上占据了以它为根的子树。这棵子树延伸到第 $L_{max}$ 层时，会覆盖 $2^{L_{max} - l_i}$ 个底层叶子节点。
3.  因为是前缀码，所有码字对应的子树互不重叠。因此，所有码字覆盖的底层叶子节点总数，不能超过树的总宽度。

$$ 
\sum_{i} 2^{L_{max} - l_i} \le 2^{L_{max}} 
$$

将不等式两边同时除以 $2^{L_{max}}$：

$$ 
\sum_{i} \frac{2^{L_{max} - l_i}}{2^{L_{max}}} \le 1 
$$

化简得到：

$$ 
\sum_{i} 2^{-l_i} \le 1 
$$

证毕。这解释了为什么 $2^{-l}$ 可以被视作一种“消耗份额”：它精确地对应了该码字在满二叉树中占据的底层比例。
