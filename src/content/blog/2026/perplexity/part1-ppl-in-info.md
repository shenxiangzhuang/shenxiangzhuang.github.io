---
title: '信息论中的Perplexity'
description: '信息论的三块基石'
date: 2026-02-08
tags: ['AI', 'NLP']
authors: ['mathew']
draft: false
order: 1
---


## 熵 (Entropy)：不确定性的尺度

熵是衡量一个概率分布 $P$ 内部“不确定性”的指标。

假设有一个离散随机变量 $X$，其可能取值为 $x_1, ..., x_n$，服从真实的概率分布 $P(x)$。香农（Shannon）定义的熵 $H(P)$ 为：

$$
H(P) = -\sum_{x} P(x) \log_2 P(x)
$$

*注：通常使用以 2 为底的对数，单位为比特（bits）。如果使用自然对数 $e$，单位为纳特（nats）。*

**关键性质与证明：**

1.  **极小值（确定性）：** 当且仅当分布集中在某一个结果上时（即某个 $P(x_k)=1$，其余为 0），熵取最小值 0。
*   **证明：** $H(P) = -1 \cdot \log_2 1 - \sum 0 \cdot \log_2 0 = 0$。（根据极限 $\lim_{x\to 0+} x \log x = 0$）。这对应了完全确定的情况，没有不确定性。

1.  **极大值（均匀性）：** 当且仅当分布为均匀分布时（即所有 $P(x_i) = 1/n$），熵取最大值 $\log_2 n$。
*   **证明（利用 Jensen 不等式）：**
    由于对数函数 $y=\log x$ 是凹函数，根据 Jensen 不等式 $E[\log X] \le \log E[X]$：
    $$
    H(P) = \sum_{i} P(x_i) \log \frac{1}{P(x_i)} \le \log \left( \sum_{i} P(x_i) \frac{1}{P(x_i)} \right) = \log \left( \sum_{i} 1 \right) = \log n
    $$
    当且仅当随机变量 $1/P(x_i)$ 为常数（即所有 $P(x_i)$ 相等）时取等号。此时不确定性最大。

## 交叉熵 (Cross-Entropy)：拟合的代价

如果说熵是上帝视角下的最优编码长度，那么交叉熵就是凡人视角的现实难度。

在现实世界中，我们往往无法得知真实的分布 $P$，只能构建一个模型分布 $Q$ 去近似它。当我们使用基于 $Q$ 优化的编码方案（比如哈夫曼编码），来处理实际上服从 $P$ 的数据时，必然会产生效率损失（即具体的编码长度会比理论最优值更长）。

**定义：**
交叉熵 $H(P, Q)$ 衡量的是：在真实分布为 $P$ 的情况下，使用模型 $Q$ 进行预测（或编码）所产生的平均惊诧度（或平均比特数）。

$$
H(P, Q) = \mathbb{E}_{x \sim P} [-\log Q(x)] = -\sum_{x} P(x) \log Q(x)
$$

**分解定理（核心直觉）：**
为什么交叉熵总是大于熵？我们可以通过简单的代数变换将其拆解为两部分：

$$
\begin{aligned}
H(P, Q) &= -\sum P(x) \log Q(x) \\
&= -\sum P(x) \log \left( P(x) \cdot \frac{Q(x)}{P(x)} \right) \\
&= \underbrace{-\sum P(x) \log P(x)}_{H(P)} + \underbrace{\sum P(x) \log \frac{P(x)}{Q(x)}}_{D_{KL}(P || Q)}
\end{aligned}
$$

即：

$$
H(P, Q) = H(P) + D_{KL}(P || Q)
$$

这个等式揭示了交叉熵的物理意义：
1.  **$H(P)$（内在不确定性）**：这是数据本身固有的熵，无论模型多强大都无法消除的底噪（Aleatoric Uncertainty）。
2.  **$D_{KL}(P || Q)$（KL 散度）**：这是因为模型 $Q$ 对真实分布 $P$ 的认知偏差而产生的**额外代价**。

**吉布斯不等式及其证明：**
我们直觉上也知道“错误的模型肯定比正确的模型差”，即 $H(P, Q) \ge H(P)$。这等价于证明 $D_{KL}(P || Q) \ge 0$。

*   **证明（利用 Jensen 不等式）：**
    考虑 KL 散度的相反数（这里取负号以便使用 $\log$ 凹函数的性质）：
    $$
    -D_{KL}(P || Q) = \sum_{x} P(x) \log \frac{Q(x)}{P(x)}
    $$
    根据 Jensen 不等式 $E[\log X] \le \log E[X]$：
    $$
    \sum_{x} P(x) \log \frac{Q(x)}{P(x)} \le \log \left( \sum_{x} P(x) \frac{Q(x)}{P(x)} \right) = \log \left( \sum_{x} Q(x) \right) = \log 1 = 0
    $$
    
    因为 $-D_{KL} \le 0$，所以 $D_{KL} \ge 0$。
    
    **结论：** 当且仅当 $Q(x) = P(x)$ 时，KL 散度为 0，交叉熵取得最小值 $H(P)$。这就是机器学习中要把 Minimize Cross Entropy 作为目标函数的原因——对于固定的数据集（$P$ 固定），最小化交叉熵等价于最小化 KL 散度，即让模型 $Q$ 逼近真相 $P$。

## 困惑度 (Perplexity)：熵的指数化

**Perplexity 这个词是香农发明的吗？**
并不是。但是 **$2^H$** 这个数学概念所蕴含的直觉，最早可以追溯到香农。

在 1948 年的奠基之作 *A Mathematical Theory of Communication* 中，香农定义了熵。虽然他主要使用“比特”（Bits）作为单位，但也阐述了 $H = \log_2 N$ 的关系：如果一个信源包含 $N$ 个**等概率**的符号，其熵为 $\log_2 N$。

反过来说，对于任意一个熵为 $H$ 的概率分布，我们都可以找到一个“等效”的均匀分布，其包含的选项数量为 $2^H$。

**Motivation（动机）：**
为什么要关注 $2^H$？因为它将抽象的“比特”还原为了线性的“可能性数量”。

- 比如一个信源的熵是 5 bits。这句话对普通人很抽象。
- 但如果计算 $2^5 = 32$，物理意义瞬间清晰：这个信源的不确定性，**等价于**从 32 个等可能的符号中随机选择一个。

这就是 Perplexity 的本质——它是**熵的指数形式**：

$$
PP = 2^{H(P, Q)}
$$

或者如果交叉熵是基于自然对数（$e$）计算的：

$$
PP = e^{H(P, Q)}
$$

所谓 **"Perplexity"** 这个名字，则是直到 1970 年代，才由 Fred Jelinek 等人在语音识别领域正式命名的。

## 物理意义：分支系数（Branching Factor）

从上面的数学公式中，我们如何获得直观的物理感受？

想象一个骰子。
- 如果是完美均匀的 6 面骰子，每个面概率是 $1/6$。
    - 熵是 $\log_2 6$。
    - 困惑度是 $2^{\log_2 6} = 6$。

- 如果是作弊骰子，如果你 100% 确定它会出 6 点。
    - 熵是 0。
    - 困惑度是 $2^0 = 1$。

**物理意义（统计学）：** 你对下一个结果的困惑程度，相当于在若干个选项中盲选；Perplexity 本质上是在衡量**“加权平均的分支因子”**。