---
title: '信息论中的Perplexity'
description: '从赛马赌局推导出的不确定性度量'
date: 2026-02-08
tags: ['AI', 'NLP']
authors: ['mathew']
draft: false
order: 1
---

Perplexity是衡量不确定性的尺度之一，其和信息论的核心概念——熵（Entropy）密切相关。在本章中，我们将从一个简单的赛马赌局出发，逐步引入熵、交叉熵和KL散度等概念，最终揭示Perplexity背后的物理意义。


## 直觉先行：赛马场的通讯课

让我们从信息论经典教材 Cover & Thomas 中的一个 赌马（Horse Race） 例子开始。

假设你正在观看一场由 8 匹马参加的比赛，你需要给远在城里的博彩公司发消息，告诉他们哪匹马赢了。为了节省昂贵的电报费，你需要设计一套最省字符的编码方案。

### 场景一：势均力敌（盲选）

如果这 8 匹马实力相当，每一匹马获胜的概率都是 1/8。
为了区分这 8 个等可能的选项，你别无选择，必须使用定长的二进制编码：

| 编号 | 马 1 | 马 2 | 马 3 | 马 4 | 马 5 | 马 6 | 马 7 | 马 8 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 编码 | `000` | `001` | `010` | `011` | `100` | `101` | `110` | `111` |

**平均长度**：无论谁赢，你都必须发送 **3 个比特**。
$$ \text{Length} = \log_2 8 = 3 \text{ bits} $$

### 场景二：强弱分明（策略）

但现实往往不是均匀的。假设你通过内幕消息得知，马匹的强弱极其悬殊，真实的获胜概率分布 $P(x)$ 如下（引自 Jurafsky & Martin 经典教材案例）：

| 编号 | 马 1 | 马 2 | 马 3 | 马 4 | 马 5 | 马 6 | 马 7 | 马 8 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 胜率 $P(x)$ | **1/2** | **1/4** | **1/8** | **1/16** | **1/64** | **1/64** | **1/64** | **1/64** |

这时候如果你对于所有马都使用 `000`-`111` 的 3 位编码就太浪费了。因为虽然你每次都发 3 个字，但绝大多数时候你发的都是"马 1"赢了的消息。

**直觉告诉我们**：既然“Horse 1”有一半的时间都在赢，为什么不给它分配一个极短的代码（比如就 1 位）？至于“Horse 8”，几百年才赢一次，代码长一点（比如 6 位）也无伤大雅，反正很少用到。

基于这个**“以此易彼”**的策略，我们可以凑出一种极其精妙的编码方案：

| 编号 | 马 1 | 马 2 | 马 3 | 马 4 | 马 5 | 马 6 | 马 7 | 马 8 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 概率 $P(x)$ | 1/2 | 1/4 | 1/8 | 1/16 | 1/64 | 1/64 | 1/64 | 1/64 |
| 巧妙的编码 | `0` | `10` | `110` | `1110` | `111100` | `111101` | `111110` | `111111` |
| 实际长度 $l_i$ | 1 | 2 | 3 | 4 | 6 | 6 | 6 | 6 |

**现在的平均长度是多少？** 让我们把每一项都加起来：

$$
\begin{aligned}
\text{Avg Length} &= \sum_{i=1}^{8} P(x_i) \cdot l_i \\
&= 1/2 \times 1 + 1/4 \times 2 + 1/8 \times 3 + 1/16 \times 4 + 4 \times (1/64 \times 6) \\
&= 0.5 + 0.5 + 0.375 + 0.25 + 0.375 \\
&= \mathbf{2 \text{ bits}}
\end{aligned}
$$

**惊人的结论**：虽然同样是 8 匹马，但在这种分布下，通过给高频事件分配短码，我们平均只需要 **2 个比特** 就能把结果传输出去。比之前的 3 个比特节省了 33%！

*敏锐的读者可能会问：Horse 3 为什么是 110 而不是 11？因为如果用了 11，那么所有 11 开头的编码（如 1110）都无法区分了。这就是“前缀码”的约束，我们必须给更小概率的事件留出编码空间。 这部分的细节在附录里有更深入的解释。*

---

## 从直觉到定义：为什么偏偏是 $-\log P(x)$？

回头审视上面那个“凑出来”的编码表，你会发现一个极其微妙的规律：
- Horse 1 概率是 $1/2$，我们给它的长度是 1。
- Horse 2 概率是 $1/4$，我们给它的长度是 2。
- Horse 5 概率是 $1/64$，我们给它的长度是 6。

似乎**最佳编码长度 $l$** 和 **概率 $P$** 之间存在着某种必然的数学映射：
$$ l = -\log_2 P $$

这是巧合吗？不。我们可以用**数学优化**的方法严格证明：这是在有限的编码资源下，使得平均长度最短的唯一解。

我们要解决的是一个**带约束的求极值问题**（记 $P_i = P(x_i)$）：

1.  **目标（Goal）**：最小化平均码长
    $$ \min \sum_{i} P_i \cdot l_i $$
2.  **约束（Constraint）**：前缀码必须满足 Kraft 不等式
    $$ \sum_{i} 2^{-l_i} \le 1 $$
    *具体证明见本文附录。*

为简洁起见，我们先把整数码长放松到实数域，并把 Kraft 不等式取等号。理由是：若最优解处有“剩余份额”，则可以整体减小所有 $l_i$ 使平均码长进一步下降，矛盾。

首先，构造拉格朗日函数:

$$
\mathcal{L}(l_1,\dots,l_n,\lambda) = \sum_i P_i \, l_i + \lambda\left(\sum_i 2^{-l_i} - 1\right)
$$

然后，对 $l_i$ 求导并令其为零：

$$
\frac{\partial \mathcal{L}}{\partial l_i} = P_i - \lambda \ln 2 \cdot 2^{-l_i} = 0
$$
$$
2^{-l_i} = c P_i, \quad c = \frac{1}{\lambda \ln 2}
$$

最后，用归一化确定常数：

$$
\sum_i 2^{-l_i} = 1 \Rightarrow c \sum_i P_i = 1 \Rightarrow c = 1
$$

因此 $2^{-l_i} = P_i$，两边取 $\log_2$ 得最优码长：

$$
\boxed{l_i = -\log_2 P_i}
$$



## 插曲：为什么编码长度等于“惊诧度”？

我们在数学上推导出了最优编码长度 $l = -\log P$。但这不仅仅是一个为了节省电报费的压缩技巧，它深刻地揭示了信息的本质。

试想当你收到那份赛马电报时：
- 如果你收到 `0`（1 bit），你知道是常胜将军“马1”赢了。这完全在你的意料之中，你的心情毫无波澜。**短编码 $\iff$ 意料之中 (Low Surprise)**。
- 如果你收到 `111111`（6 bits），这意味着是一匹万年吊车尾的“马8”赢了。你会大吃一惊：“什么？居然是它？” **长编码 $\iff$ 出乎意料 (High Surprise)**。

这并非巧合。Shannon 在 1948 年那篇开创性的论文 [*A Mathematical Theory of Communication*](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) 中，通过**“源编码定理” (Source Coding Theorem)** 严格证明了一个令人震惊的事实：

**“信息”的抽象度量与“编码”的物理极限是同一回事。**

这并非 Shannon 的原话，而是对他著名的 **“源编码定理” (Source Coding Theorem)** ——即论文中 **Theorem 9** 的现代解读。

在论文的 Part I, Section 9 中，Shannon 写道：

> **Theorem 9**: Let a source have entropy $H$ (bits per symbol) ... it is possible to encode the output of the source in such a way as to transmit at the average rate $\frac{C}{H} - \epsilon$ symbols per second ... where $\epsilon$ is arbitrarily small. It is not possible to transmit at an average rate greater than $\frac{C}{H}$.

换用今天更通用的表述（针对无噪声信道）：
> 对于熵为 $H(X)$ 的信源，存在一种编码方案，使得其平均码长 $L$ 满足：
> $$ H(X) \le L < H(X) + \epsilon $$
> 且无法找到任何一种方案使得 $L < H(X)$。

这一数学定理如有神谕般地指出：**熵（这一基于概率定义的抽象量）恰好就是数据压缩的物理下界**。

基于这个完美的对应，我们将量 $I(x) = -\log_2 P(x)$ 定义为事件 $x$ 的**自信息 (Self-information)** 或 **惊诧度 (Surprisal)**：
- 当 $P(x) = 1$ (100% 发生): $-\log 1 = 0$。太阳从东方升起，毫无意外，信息量为 0，编码长度为 0。
- 当 $P(x) \to 0$ (极小概率): $-\log P \to \infty$。彗星撞地球，惊天大新闻，信息量无限大。

正是基于这个定义，我们才能自然地迈向下一个概念——衡量整个系统“平均惊诧度”的熵。

---

## 熵 (Entropy)：期望的惊诧度

有了前面的铺垫，我们可以给出**熵**（Entropy）的严格数学定义。

设 $X$ 是一个离散随机变量，其取值空间为 $\mathcal{X}$，概率分布为 $P(x) = \Pr(X=x)$。那么，$X$ 的熵 $H(X)$ 定义为：

$$
H(X) \triangleq -\sum_{x \in \mathcal{X}} P(x) \log_2 P(x)
$$

*注：若 $P(x)=0$，依据极限 $\lim_{p\to 0} p \log p = 0$，定义 $0 \log 0 = 0$。*

从数学形式上看，$H(X)$ 是随机变量 $X$ 的函数；但本质上，**熵只依赖于分布 $P$ 本身**，常记作 $H(P)$。

**如何直观理解这个公式？**

如果说 $-\log P(x)$ 刻画了**单个事件发生时的“惊诧度”**（Surprisal），那么**熵**只不过是衡量**整个系统的“平均惊诧度”**（Expected Surprisal）。它正是惊诧度这一随机变量的**数学期望**：

$$
H(P) = \mathbb{E}_{x \sim P} [-\log P(x)]
$$

这就将熵的物理意义从“最小平均编码长度”推广到了更普适的不确定性度量。让我们重温赛马场的两个平行宇宙，看看熵是如何精准捕捉这种“系统级不确定性”的：

- **场景一（完全随机）**：
  8 匹马胜率相等。无论谁赢，带给你的“信息量”都是一样的（3 bits）。因为没有“倾向性”，系统充满了最大的不确定性，**平均惊诧度（熵）达到最大值 3 bits**。

- **场景二（强弱分明）**：
  Horse 1 众望所归（$P=1/2$），它赢了你毫不意外（惊诧度仅 1 bit）；Horse 8 爆冷获胜（$P=1/64$）虽然会让你大跌眼镜（惊诧度高达 6 bits），但这种事**极少发生**。
  熵不仅看“惊诧度”，更看“权重”。当你把高频的“平淡”和罕见的“震惊”加权平均后，**整体的平均惊诧度（熵）反而下降到了 2 bits**。

**这就是熵的洞见：**
熵越低，意味着系统越“有序”，大部分时候发生的事情都在意料之中（概率集中在少数选项）；熵越高，意味着系统越“混乱”，各种可能性势均力敌，让你难以预测。

**简言之，熵就是你对下一次结果的“期望迷茫程度”。**


## 交叉熵 (Cross-Entropy)：认知的代价

在现实中，我们往往不知道真实的马匹胜率 $P(x)$。博彩公司会根据经验估计出一个概率分布 $Q(x)$（即**模型分布**）。

当我们制定编码策略时，我们只能基于**我们的认知 $Q$** 来设定长度：我们认为 $x$ 发生的概率是 $Q(x)$，所以我们给它分配长度为 $-\log Q(x)$ 的代码。

但是，**现实是残酷的**。实际比赛的结果是按照**真实分布 $P(x)$** 发生的。

于是，实际的平均传输长度就变成了：

$$
H(P, Q) = \sum_{x} \underbrace{P(x)}_{\text{Reality}} \cdot \underbrace{\left( -\log_2 Q(x) \right)}_{\text{Our Strategy}}
$$

这就是 **交叉熵**。

如果我们猜对了（$Q=P$），那么交叉熵等于熵（$H(P,Q) = H(P)$），我们达到了理论最优。
如果我们猜错了（比如还是那个偏斜的赛马局，但我们误判为所有马胜率相等），我们就会给“马1”分配 3 bit (000)，而不是 1 bit (0)。

这中间的差值，就是 **KL 散度（Relative Entropy）**，即**无知/误判带来的额外代价**：

$$
H(P, Q) = H(P) + D_{KL}(P || Q)
$$

## 困惑度 (Perplexity)：从比特回归选项

在这个漫长的旅程最后，我们终于来到了 **Perplexity (PPL)**。

作为人类，我们对“比特”其实很不敏感。
- “这个模型的熵是 2 bit。” —— 听起来没什么感觉。
- “这个模型的熵是 5 bit。” —— 比上面那个大，但大多少？

Fred Jelinek 等人提出的 PPL，旨在把抽象的“比特数”还原回直观的“**选项数**”:

$$
\text{PPL} = 2^{H(P, Q)} \quad (\text{或者 } e^{H(P, Q)})
$$

那么，PPL = 4 意味着什么？让我们回到赛马的例子。
- **场景一**：熵 = 3 bits。PPL = $2^3 = 8$。
    - **物理意义**：这相当于在 8 匹势均力敌的马里猜，难度是 **8 选 1**。
- **场景二**：熵 = 2 bits。PPL = $2^2 = 4$。
    - **物理意义**：虽然场上还是 8 匹马，但由于强弱极其分明，你的预测难度大大降低了。这就**“等效于”**在一场只有 **4 匹** 势均力敌的马的比赛中猜赢家。

---

## 附录

### 为什么 Horse 3 不能用 `11`?

在正文中我们提到，Horse 3 的编码是 `110`（长度 3），而不是更短的 `11`（长度 2）。这背后的原因可以从两个互为表里的角度来解释：**解码的歧义性**和**空间的排他性**。

1. 拒绝歧义：前缀性质 (Prefix Property)

前缀码要求**任何一个字符的编码，都不能是另一个字符编码的前缀。** 只有这样，接收方才能在收到比特流时瞬间解码（Instant Decodability）。

*   **反面教材**：假设 Horse 3 是 `11`，Horse 4 是 `1110`。
    当接收方收到比特流 `1110...` 时，读到前两位 `11`，由于 `11` 既可能是 Horse 3 的全部，也可能是 Horse 4 的开头，接收方无法立刻判断，必须“往后看”。这违背了即时解码的原则。

2. 拒绝“德不配位”：空间份额守恒

更本质的解释依然是**“分蛋糕”**。编码空间总量是有限的（总和为 1）。

*   Horse 1 (`0`) 占用了 **1/2** 的空间。
*   Horse 2 (`10`) 占用了 **1/4** 的空间。
*   **如果 Horse 3 强行占用 `11`**：
    *   它是长度为 2 的编码，会消耗 **1/4** 的编码空间（$2^{-2} = 1/4$）。
    *   但这比它实际拥有的 **1/8** 概率要大，相当于它**“德不配位”**，占用了过多的资源。
    *   结果是：$1/2 + 1/4 + 1/4 = 1$。整个编码空间被填满了！Horse 4, 5, 6, 7, 8 将**无路可走**。

为了给后面的“穷兄弟”留条活路，Horse 3 只能克制一点，占用 `110`（空间 $1/8$），从而把 `111` 这个前缀（空间 $1/8$）剩下来，留给 Horse 4 及其后续的马去瓜分。

这就是为什么**概率**和**长度**必须严格对应（$2^{-l} \le P$）的物理原因：**你不仅代表你自己，你还要为比你概率更小的事件腾出空间。**

### Kraft 不等式的数学证明（二叉树模型）
我们可以用一颗**二叉树**来描述所有可能的二级制编码：
- 根节点代表空。
- 向左走代表 `0`，向右走代表 `1`。
- 每一个合法的码字，都是这就棵树上的一个**叶子节点**（或者被标记为已占用的终止节点）。

**前缀性质**意味着：如果一个节点被选为码字，那么它的所有子树（子孙节点）都不能再被使用。

证明思路如下：
假设我们只允许最长编码长度为 $L_{max}$。我们可以构建一颗深度为 $L_{max}$ 的满二叉树。
我们可以把每个码字 $l_i$ 都投影到最底层的第 $L_{max}$ 层去计算“投影面积”：
1.  第 $L_{max}$ 层总共有 $2^{L_{max}}$ 个叶子节点。
2.  对于任意一个长度为 $l_i$ 的码字，它实际上占据了以它为根的子树。这棵子树延伸到第 $L_{max}$ 层时，会覆盖 $2^{L_{max} - l_i}$ 个底层叶子节点。
3.  因为是前缀码，所有码字对应的子树互不重叠。因此，所有码字覆盖的底层叶子节点总数，不能超过树的总宽度。

$$ 
\sum_{i} 2^{L_{max} - l_i} \le 2^{L_{max}} 
$$

将不等式两边同时除以 $2^{L_{max}}$：

$$ 
\sum_{i} \frac{2^{L_{max} - l_i}}{2^{L_{max}}} \le 1 
$$

化简得到：

$$ 
\sum_{i} 2^{-l_i} \le 1 
$$

证毕。这解释了为什么 $2^{-l}$ 可以被视作一种“消耗份额”：它精确地对应了该码字在满二叉树中占据的底层比例。
