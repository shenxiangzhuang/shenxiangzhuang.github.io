---
title: '平均最小编码长度'
description: '平均最小编码长度与熵的诞生'
date: 2026-02-08
tags: ['AI', 'Information Theory']
authors: ['mathew']
draft: false
order: 2
---


## 直觉先行：赛马场的通讯课

让我们从信息论经典教材 Cover & Thomas 中的一个 赌马（Horse Race） 例子开始。

假设你正在观看一场由 8 匹马参加的比赛，你需要给远在城里的博彩公司发消息，告诉他们哪匹马赢了。为了节省昂贵的电报费，你需要设计一套最省字符的编码方案。

### 场景一：势均力敌（盲选）

如果这 8 匹马实力相当，每一匹马获胜的概率都是 1/8。
为了区分这 8 个等可能的选项，你别无选择，必须使用定长的二进制编码：

| 编号 | 马 1 | 马 2 | 马 3 | 马 4 | 马 5 | 马 6 | 马 7 | 马 8 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 编码 | `000` | `001` | `010` | `011` | `100` | `101` | `110` | `111` |

**平均长度**：无论谁赢，你都必须发送 **3 个比特**。
$$ \text{Length} = \log_2 8 = 3 \text{ bits} $$

### 场景二：强弱分明（策略）

但现实往往不是均匀的。假设你通过内幕消息得知，马匹的强弱极其悬殊，真实的获胜概率分布 $P(x)$ 如下（引自 Jurafsky & Martin 经典教材案例）：

| 编号 | 马 1 | 马 2 | 马 3 | 马 4 | 马 5 | 马 6 | 马 7 | 马 8 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 胜率 $P(x)$ | **1/2** | **1/4** | **1/8** | **1/16** | **1/64** | **1/64** | **1/64** | **1/64** |

这时候如果你对于所有马都使用 `000`-`111` 的 3 位编码就太浪费了。因为虽然你每次都发 3 个字，但绝大多数时候你发的都是"马 1"赢了的消息。

**直觉告诉我们**：既然“Horse 1”有一半的时间都在赢，为什么不给它分配一个极短的代码（比如就 1 位）？至于“Horse 8”，几百年才赢一次，代码长一点（比如 6 位）也无伤大雅，反正很少用到。

基于这个**“以此易彼”**的策略，我们可以凑出一种极其精妙的编码方案：

| 编号 | 马 1 | 马 2 | 马 3 | 马 4 | 马 5 | 马 6 | 马 7 | 马 8 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 概率 $P(x)$ | 1/2 | 1/4 | 1/8 | 1/16 | 1/64 | 1/64 | 1/64 | 1/64 |
| 巧妙的编码 | `0` | `10` | `110` | `1110` | `111100` | `111101` | `111110` | `111111` |
| 实际长度 $l_i$ | 1 | 2 | 3 | 4 | 6 | 6 | 6 | 6 |

**现在的平均长度是多少？** 让我们把每一项都加起来：

$$
\begin{aligned}
\text{Avg Length} &= \sum_{i=1}^{8} P(x_i) \cdot l_i \\
&= 1/2 \times 1 + 1/4 \times 2 + 1/8 \times 3 + 1/16 \times 4 + 4 \times (1/64 \times 6) \\
&= 0.5 + 0.5 + 0.375 + 0.25 + 0.375 \\
&= \mathbf{2 \text{ bits}}
\end{aligned}
$$

**惊人的结论**：虽然同样是 8 匹马，但在这种分布下，通过给高频事件分配短码，我们平均只需要 **2 个比特** 就能把结果传输出去。比之前的 3 个比特节省了 33%！

*敏锐的读者可能会问：Horse 3 为什么是 110 而不是 11？因为如果用了 11，那么所有 11 开头的编码（如 1110）都无法区分了。这就是“前缀码”的约束，我们必须给更小概率的事件留出编码空间。 这部分的细节在附录里有更深入的解释。*

---

## 从直觉到定义：为什么偏偏是 $-\log P(x)$？

回头审视上面那个“凑出来”的编码表，你会发现一个极其微妙的规律：
- Horse 1 概率是 $1/2$，我们给它的长度是 1。
- Horse 2 概率是 $1/4$，我们给它的长度是 2。
- Horse 5 概率是 $1/64$，我们给它的长度是 6。

似乎**最佳编码长度 $l$** 和 **概率 $P$** 之间存在着某种必然的数学映射：
$$ l = -\log_2 P $$

这是巧合吗？不。我们可以用**数学优化**的方法严格证明：这是在有限的编码资源下，使得平均长度最短的唯一解。

我们要解决的是一个**带约束的求极值问题**（记 $P_i = P(x_i)$）：

1.  **目标（Goal）**：最小化平均码长
    $$ \min \sum_{i} P_i \cdot l_i $$
2.  **约束（Constraint）**：前缀码必须满足 Kraft 不等式
    $$ \sum_{i} 2^{-l_i} \le 1 $$
    *具体证明见本文附录。*

为简洁起见，我们先把整数码长放松到实数域，并把 Kraft 不等式取等号。理由是：若最优解处有“剩余份额”，则可以整体减小所有 $l_i$ 使平均码长进一步下降，矛盾。

首先，构造拉格朗日函数:

$$
\mathcal{L}(l_1,\dots,l_n,\lambda) = \sum_i P_i \, l_i + \lambda\left(\sum_i 2^{-l_i} - 1\right)
$$

然后，对 $l_i$ 求导并令其为零：

$$
\frac{\partial \mathcal{L}}{\partial l_i} = P_i - \lambda \ln 2 \cdot 2^{-l_i} = 0
$$
$$
2^{-l_i} = c P_i, \quad c = \frac{1}{\lambda \ln 2}
$$

最后，用归一化确定常数：

$$
\sum_i 2^{-l_i} = 1 \Rightarrow c \sum_i P_i = 1 \Rightarrow c = 1
$$

因此 $2^{-l_i} = P_i$，两边取 $\log_2$ 得最优码长：

$$
\boxed{l_i = -\log_2 P_i}
$$


---

## 平均最小编码长度

现在我们手里有两块拼图：
1.  事件 $x$ 发生的概率是 $P(x)$。
2.  为了传输效率最高，事件 $x$ 的最佳编码长度应该是 $l(x) = -\log_2 P(x)$。

那么，发送一条消息的**平均预期长度**（Average Expected Length）是多少呢？

这很简单，就是把所有可能情况的长度加权平均：

$$
\text{Average Length} = \sum_{i} P(x_i) \cdot l_i
$$

代入我们推导出的最优长度 $l_i = -\log_2 P(x_i)$：

$$
\text{Average Length} = \sum_{i} P(x_i) \cdot (-\log_2 P(x_i))
$$

把负号提出来，这正是香农熵（Shannon Entropy）的定义公式：

$$
\boxed{ H(X) = - \sum_{i} P(x_i) \log_2 P(x_i) }
$$

**至此，我们完成了闭环：**

> **熵（Entropy）的物理意义**：
> 在一个信源中，为了无损地传输信息，平均每个符号所需要的**最小比特数**。

-   如果系统很确定（比如 $P=(1, 0, \dots)$），熵为 0，你不需要发送任何比特就知道结果。
-   如果系统很混乱（比如前文的赛马例子，如果是均匀分布，熵为 3；如果是那组非均匀分布，熵为 2）。

熵，度量的就是这种**“消除不确定性所需的平均信息量”**。

---

## 附录

### 为什么 Horse 3 不能用 `11`?

在正文中我们提到，Horse 3 的编码是 `110`（长度 3），而不是更短的 `11`（长度 2）。这背后的原因可以从两个互为表里的角度来解释：**解码的歧义性**和**空间的排他性**。

1. 拒绝歧义：前缀性质 (Prefix Property)

前缀码要求**任何一个字符的编码，都不能是另一个字符编码的前缀。** 只有这样，接收方才能在收到比特流时瞬间解码（Instant Decodability）。

*   **反面教材**：假设 Horse 3 是 `11`，Horse 4 是 `1110`。
    当接收方收到比特流 `1110...` 时，读到前两位 `11`，由于 `11` 既可能是 Horse 3 的全部，也可能是 Horse 4 的开头，接收方无法立刻判断，必须“往后看”。这违背了即时解码的原则。

2. 拒绝“德不配位”：空间份额守恒

更本质的解释依然是**“分蛋糕”**。编码空间总量是有限的（总和为 1）。

*   Horse 1 (`0`) 占用了 **1/2** 的空间。
*   Horse 2 (`10`) 占用了 **1/4** 的空间。
*   **如果 Horse 3 强行占用 `11`**：
    *   它是长度为 2 的编码，会消耗 **1/4** 的编码空间（$2^{-2} = 1/4$）。
    *   但这比它实际拥有的 **1/8** 概率要大，相当于它**“德不配位”**，占用了过多的资源。
    *   结果是：$1/2 + 1/4 + 1/4 = 1$。整个编码空间被填满了！Horse 4, 5, 6, 7, 8 将**无路可走**。

为了给后面的“穷兄弟”留条活路，Horse 3 只能克制一点，占用 `110`（空间 $1/8$），从而把 `111` 这个前缀（空间 $1/8$）剩下来，留给 Horse 4 及其后续的马去瓜分。

这就是为什么**概率**和**长度**必须严格对应（$2^{-l} \le P$）的物理原因：**你不仅代表你自己，你还要为比你概率更小的事件腾出空间。**

### Kraft 不等式的数学证明（二叉树模型）
我们可以用一颗**二叉树**来描述所有可能的二级制编码：
- 根节点代表空。
- 向左走代表 `0`，向右走代表 `1`。
- 每一个合法的码字，都是这就棵树上的一个**叶子节点**（或者被标记为已占用的终止节点）。

**前缀性质**意味着：如果一个节点被选为码字，那么它的所有子树（子孙节点）都不能再被使用。

证明思路如下：
假设我们只允许最长编码长度为 $L_{max}$。我们可以构建一颗深度为 $L_{max}$ 的满二叉树。
我们可以把每个码字 $l_i$ 都投影到最底层的第 $L_{max}$ 层去计算“投影面积”：
1.  第 $L_{max}$ 层总共有 $2^{L_{max}}$ 个叶子节点。
2.  对于任意一个长度为 $l_i$ 的码字，它实际上占据了以它为根的子树。这棵子树延伸到第 $L_{max}$ 层时，会覆盖 $2^{L_{max} - l_i}$ 个底层叶子节点。
3.  因为是前缀码，所有码字对应的子树互不重叠。因此，所有码字覆盖的底层叶子节点总数，不能超过树的总宽度。

$$ 
\sum_{i} 2^{L_{max} - l_i} \le 2^{L_{max}} 
$$

将不等式两边同时除以 $2^{L_{max}}$：

$$ 
\sum_{i} \frac{2^{L_{max} - l_i}}{2^{L_{max}}} \le 1 
$$

化简得到：

$$ 
\sum_{i} 2^{-l_i} \le 1 
$$

证毕。这解释了为什么 $2^{-l}$ 可以被视作一种“消耗份额”：它精确地对应了该码字在满二叉树中占据的底层比例。
