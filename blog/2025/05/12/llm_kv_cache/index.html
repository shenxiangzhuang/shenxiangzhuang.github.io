<!doctype html><html lang=zh class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="介绍 LLM KV Cache 的原理和实现
"><meta name=author content="Xiangzhuang Shen"><link href=https://shenxiangzhuang.github.io/blog/2025/05/12/llm_kv_cache/ rel=canonical><link href=../../08/toyrl/ rel=prev><link rel=alternate type=application/rss+xml title="RSS 订阅" href=../../../../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="已更新内容的 RSS 订阅" href=../../../../../feed_rss_updated.xml><link rel=icon href=../../../../../assets/logo/glimpse-ai-logo-small.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.5.42"><title>LLM KV Cache: A Simple Implementation - Data Honor</title><link rel=stylesheet href=../../../../../assets/stylesheets/main.0253249f.min.css><link rel=stylesheet href=../../../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../../../css/neoteroi-mkdocs.css><script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-KEEHE7VY5K"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-KEEHE7VY5K",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-KEEHE7VY5K",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>if("undefined"!=typeof __md_analytics){var consent=__md_get("__consent");consent&&consent.analytics&&__md_analytics()}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=teal data-md-color-accent=deep-purple> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#introduction class=md-skip> 跳转至 </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label=不再显示此消息> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> <center> You can follow by <a href=https://datahonor.com/feed_rss_created> <strong>RSS</strong> </a> or <a href=https://github.com/shenxiangzhuang> <span class=twemoji> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M448 96c0-35.3-28.7-64-64-64H64C28.7 32 0 60.7 0 96v320c0 35.3 28.7 64 64 64h320c35.3 0 64-28.7 64-64V96zM265.8 407.7c0-1.8 0-6 .1-11.6.1-11.4.1-28.8.1-43.7 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6s-28.4 1.9-41.6 5.6c0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 9 .1 21.7.1 30.6 0 4.8.1 8.6.1 10 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3-8.4 1.5-11.5-3.7-11.5-8zm-90.5-54.8c-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3-1.9.4-3.7-.4-3.9-1.7zm-9.1 3.2c-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4 0 1.3-1.5 2.4-3.5 2.4zm-14.3-2.2c-1.9-.4-3.2-1.9-2.8-3.2s2.4-1.9 4.1-1.5c2 .6 3.3 2.1 2.8 3.4-.4 1.3-2.4 1.9-4.1 1.3zm-12.5-7.3c-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1-.9 1.1-2.8.9-4.3-.6zm-8.5-10c-1.1-1.5-1.1-3.2 0-3.9 1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1-.9.6-2.6 0-3.7-1.5zm-6.3-8.8c-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5-.9.9-2.4.4-3.5-.6zm-6-6.4c-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6-.4.9-1.7 1.1-2.8.4z"/></svg> </span> <strong>GitHub(<strong>shenxiangzhuang</strong>)</strong> </a> <!--  or--> <!--  <a href="https://www.zhihu.com/people/shen-xiang-zhuang">--> <!--    <strong>知乎</strong>--> <!--  </a>--> or <!--暂时用 LLM in 2024 文章链接作为入口--> <a href=https://mp.weixin.qq.com/s/wOqp6nHBAenK9wP2vIUS9g> <span class=twemoji> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 4C5.36 4 2 6.69 2 10c0 1.89 1.08 3.56 2.78 4.66L4 17l2.5-1.5c.89.31 1.87.5 2.91.5A5.22 5.22 0 0 1 9 14c0-3.31 3.13-6 7-6 .19 0 .38 0 .56.03C15.54 5.69 12.78 4 9.5 4m-3 2.5a1 1 0 0 1 1 1 1 1 0 0 1-1 1 1 1 0 0 1-1-1 1 1 0 0 1 1-1m5 0a1 1 0 0 1 1 1 1 1 0 0 1-1 1 1 1 0 0 1-1-1 1 1 0 0 1 1-1M16 9c-3.31 0-6 2.24-6 5s2.69 5 6 5c.67 0 1.31-.08 1.91-.25L20 20l-.62-1.87C20.95 17.22 22 15.71 22 14c0-2.76-2.69-5-6-5m-2 2.5a1 1 0 0 1 1 1 1 1 0 0 1-1 1 1 1 0 0 1-1-1 1 1 0 0 1 1-1m4 0a1 1 0 0 1 1 1 1 1 0 0 1-1 1 1 1 0 0 1-1-1 1 1 0 0 1 1-1Z"/></svg> </span> <strong>WeChat(<strong>AI Glimpse</strong>)</strong> </a> </center> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=页眉> <a href=../../../../.. title="Data Honor" class="md-header__button md-logo" aria-label="Data Honor" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M6.27 17.05A2.991 2.991 0 0 1 4 22c-1.66 0-3-1.34-3-3s1.34-3 3-3c.18 0 .36 0 .53.05l3.07-5.36-1.74-.99 4.09-1.12 1.12 4.09-1.74-.99zM20 16c-1.3 0-2.4.84-2.82 2H11v-2l-3 3 3 3v-2h6.18c.42 1.16 1.52 2 2.82 2 1.66 0 3-1.34 3-3s-1.34-3-3-3m-8-8c.18 0 .36 0 .53-.05l3.07 5.36-1.74.99 4.09 1.12 1.12-4.09-1.74.99-3.06-5.37A2.991 2.991 0 0 0 12 2c-1.66 0-3 1.34-3 3s1.34 3 3 3"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Data Honor </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> LLM KV Cache: A Simple Implementation </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=teal data-md-color-accent=deep-purple aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=cyan data-md-color-accent=deep-purple aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=搜索 placeholder=搜索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=查找> <a href=javascript:void(0) class="md-search__icon md-icon" title=分享 aria-label=分享 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=清空当前内容 aria-label=清空当前内容 tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/shenxiangzhuang/shenxiangzhuang.github.io title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> Xiangzhuang Shen'S Blog </div> </a> </div> </nav> <nav class=md-tabs aria-label=标签 data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../../.. class=md-tabs__link> About </a> </li> <li class=md-tabs__item> <a href=../../../../../datascience/statistics/ class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../../../../../cs/programming/ class=md-tabs__link> Computer Science </a> </li> <li class=md-tabs__item> <a href=../../../../../se/ class=md-tabs__link> Software Engineering </a> </li> <li class=md-tabs__item> <a href=../../../../../odyssey/ class=md-tabs__link> Career Odyssey </a> </li> <li class=md-tabs__item> <a href=../../../../../tool/ class=md-tabs__link> Tool </a> </li> <li class=md-tabs__item> <a href=../../../../../project/ class=md-tabs__link> Project </a> </li> <li class=md-tabs__item> <a href=../../../../../life/ class=md-tabs__link> Life </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../../ class=md-tabs__link> Blog </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=导航栏 data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../../.. title="Data Honor" class="md-nav__button md-logo" aria-label="Data Honor" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M6.27 17.05A2.991 2.991 0 0 1 4 22c-1.66 0-3-1.34-3-3s1.34-3 3-3c.18 0 .36 0 .53.05l3.07-5.36-1.74-.99 4.09-1.12 1.12 4.09-1.74-.99zM20 16c-1.3 0-2.4.84-2.82 2H11v-2l-3 3 3 3v-2h6.18c.42 1.16 1.52 2 2.82 2 1.66 0 3-1.34 3-3s-1.34-3-3-3m-8-8c.18 0 .36 0 .53-.05l3.07 5.36-1.74.99 4.09 1.12 1.12-4.09-1.74.99-3.06-5.37A2.991 2.991 0 0 0 12 2c-1.66 0-3 1.34-3 3s1.34 3 3 3"/></svg> </a> Data Honor </label> <div class=md-nav__source> <a href=https://github.com/shenxiangzhuang/shenxiangzhuang.github.io title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> Xiangzhuang Shen'S Blog </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../.. class=md-nav__link> <span class=md-ellipsis> About </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../datascience/statistics/ class=md-nav__link> <span class=md-ellipsis> Data Science </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../cs/programming/ class=md-nav__link> <span class=md-ellipsis> Computer Science </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../se/ class=md-nav__link> <span class=md-ellipsis> Software Engineering </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../odyssey/ class=md-nav__link> <span class=md-ellipsis> Career Odyssey </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../tool/ class=md-nav__link> <span class=md-ellipsis> Tool </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../project/ class=md-nav__link> <span class=md-ellipsis> Project </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../life/ class=md-nav__link> <span class=md-ellipsis> Life </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9 checked> <div class="md-nav__link md-nav__container"> <a href=../../../../ class="md-nav__link "> <span class=md-ellipsis> Blog </span> </a> <label class="md-nav__link " for=__nav_9 id=__nav_9_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_9_label aria-expanded=true> <label class=md-nav__title for=__nav_9> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9_2> <label class=md-nav__link for=__nav_9_2 id=__nav_9_2_label tabindex> <span class=md-ellipsis> 归档 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_9_2_label aria-expanded=false> <label class=md-nav__title for=__nav_9_2> <span class="md-nav__icon md-icon"></span> 归档 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../archive/2025/ class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2022/ class=md-nav__link> <span class=md-ellipsis> 2022 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2021/ class=md-nav__link> <span class=md-ellipsis> 2021 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2019/ class=md-nav__link> <span class=md-ellipsis> 2019 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2018/ class=md-nav__link> <span class=md-ellipsis> 2018 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9_3> <label class=md-nav__link for=__nav_9_3 id=__nav_9_3_label tabindex> <span class=md-ellipsis> 分类 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_9_3_label aria-expanded=false> <label class=md-nav__title for=__nav_9_3> <span class="md-nav__icon md-icon"></span> 分类 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../category/ai/ class=md-nav__link> <span class=md-ellipsis> AI </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/engineering/ class=md-nav__link> <span class=md-ellipsis> Engineering </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/llm/ class=md-nav__link> <span class=md-ellipsis> LLM </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/mle/ class=md-nav__link> <span class=md-ellipsis> MLE </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/odyssey/ class=md-nav__link> <span class=md-ellipsis> Odyssey </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/paper/ class=md-nav__link> <span class=md-ellipsis> Paper </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/project/ class=md-nav__link> <span class=md-ellipsis> Project </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/statistics/ class=md-nav__link> <span class=md-ellipsis> Statistics </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E4%B9%A6%E8%AF%84/ class=md-nav__link> <span class=md-ellipsis> 书评 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E5%9B%9E%E5%BF%86%E5%BD%95/ class=md-nav__link> <span class=md-ellipsis> 回忆录 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E5%BC%80%E6%BA%90/ class=md-nav__link> <span class=md-ellipsis> 开源 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/ class=md-nav__link> <span class=md-ellipsis> 数据分析 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E6%96%87%E5%AD%A6/ class=md-nav__link> <span class=md-ellipsis> 文学 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E6%97%A5%E8%AE%B0/ class=md-nav__link> <span class=md-ellipsis> 日记 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E7%AE%97%E6%B3%95/ class=md-nav__link> <span class=md-ellipsis> 算法 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E7%BB%88%E8%BA%AB%E5%AD%A6%E4%B9%A0/ class=md-nav__link> <span class=md-ellipsis> 终身学习 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E9%9A%8F%E7%AC%94/ class=md-nav__link> <span class=md-ellipsis> 随笔 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#introduction class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=#what class=md-nav__link> <span class=md-ellipsis> What </span> </a> </li> <li class=md-nav__item> <a href=#why-how class=md-nav__link> <span class=md-ellipsis> Why &amp; How </span> </a> </li> <li class=md-nav__item> <a href=#code class=md-nav__link> <span class=md-ellipsis> Code </span> </a> </li> <li class=md-nav__item> <a href=#the-end class=md-nav__link> <span class=md-ellipsis> The end </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-content md-content--post" data-md-component=content> <div class="md-sidebar md-sidebar--post" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class="md-sidebar__inner md-post"> <nav class="md-nav md-nav--primary"> <div class=md-post__back> <div class="md-nav__title md-nav__container"> <a href=../../../../ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> <span class=md-ellipsis> 回到主页 </span> </a> </div> </div> <div class="md-post__authors md-typeset"> <div class="md-profile md-post__profile"> <span class="md-author md-author--long"> <img src=https://avatars.githubusercontent.com/u/17157965 alt="Xiangzhuang Shen"> </span> <span class=md-profile__description> <strong> <a href=https://github.com/shenxiangzhuang>Xiangzhuang Shen</a> </strong> <br> Creator </span> </div> </div> <ul class="md-post__meta md-nav__list"> <li class="md-nav__item md-nav__item--section"> <div class=md-post__title> <span class=md-ellipsis> 元数据 </span> </div> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg> <time datetime="2025-05-12 00:00:00" class=md-ellipsis>2025年5月12日</time> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg> <span class=md-ellipsis> 分类于 <a href=../../../../category/%E7%BB%88%E8%BA%AB%E5%AD%A6%E4%B9%A0/ >终身学习</a>, <a href=../../../../category/llm/ >LLM</a></span> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg> <span class=md-ellipsis> 需要 8 分钟阅读时间 </span> </div> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <article class="md-content__inner md-typeset"> <a href=https://github.com/shenxiangzhuang/shenxiangzhuang.github.io/edit/master/docs/blog/posts/llm_kv_cache.md title=编辑此页 class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/shenxiangzhuang/shenxiangzhuang.github.io/raw/master/docs/blog/posts/llm_kv_cache.md title=查看本页的源代码 class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <h1>LLM KV Cache: A Simple Implementation</h1> <div><h2 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">¶</a></h2> <p>在看很多大语言模型的推理代码时，发现有一个非常重要的概念，就是 KV Cache。 这里我们简要介绍一下 KV Cache 的核心原理并给出基于 GPT-2 的代码实现以便于本地复现。 相关的实验和测试代码同样开源在<a href=https://github.com/ai-glimpse/toyllm>toyllm</a>.</p> <!-- more --> <h2 id=what>What<a class=headerlink href=#what title="Permanent link">¶</a></h2> <ul> <li>全称是 Key-Value Cache</li> <li>这里的 Key 和 Value 是 Transformer/Attention 中的 Key 和 Value</li> <li>一种空间换时间的优化策略，主要是为了加速大语言模型推理速度</li> <li>作用是缓存模型在推理过程中计算出的中间结果，以便在后续的推理中复用这些结果，从而减少计算量</li> </ul> <h2 id=why-how>Why &amp; How<a class=headerlink href=#why-how title="Permanent link">¶</a></h2> <p>目前 LLM 的核心架构都是 Decoder Only 结构 (只用了原始 Transformer 的 Decoder 部分)，其核心结构是基于 Attention 的， 而 Attention 的核心计算是基于<code>query</code>，<code>key</code>和<code>value</code>的矩阵运算。而 KV Cache 就是通过加速 Attention 的矩阵计算来加速整个模型的推理速度。具体来说，是通过缓存<code>key</code>和<code>value</code>的值来避免重复计算，从而减少计算量。那么一个核心的问题就是原始的 Attention 计算中存在哪些重复计算？下面我们将通过数学推导凸显 LLM <strong>推理阶段原始 Attention 的重复计算问题</strong>。 在重复计算的问题被凸显出来之后，KV Cache 的实现原理也就显而易见了。 下面的推导主要参考了 Lei Mao 的博客： <a href=https://leimao.github.io/article/Transformer-Autoregressive-Inference-Optimization/ >Transformer Autoregressive Inference Optimization</a>.</p> <p>我们先简要回顾一下原始的 Attention 计算过程：</p> <div class=arithmatex>\[ \begin{align} Attention(Q, K, V) = \text{softmax}(\text{Mask}(\frac{QK^T}{\sqrt{d_k}}))V \end{align} \]</div> <figure> <img alt src=../../../../images/llm_kv_cache/attention.png width=800> <figcaption>原始 Attention 计算过程。图片来源：<cite>Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models</cite></figcaption> </figure> <p>在 LLM 推理阶段 (自回归生成，Auto-regressive Generation)，在第 <span class=arithmatex>\(n+1\)</span> 个 token 的生成过程中会做如下计算：</p> <div class=arithmatex>\[ \begin{align} Q_n = X_n W^Q \in \mathbb{R}^{n \times d_k} \end{align} \]</div> <div class=arithmatex>\[ \begin{align} K_n = X_n W^K \in \mathbb{R}^{n \times d_k} \end{align} \]</div> <div class=arithmatex>\[ \begin{align} V_n = X_n W^V \in \mathbb{R}^{n \times d_v} \end{align} \]</div> <p>其中<span class=arithmatex>\(W^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>, <span class=arithmatex>\(W^K \in \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>, <span class=arithmatex>\(W^V \in \mathbb{R}^{d_{\text{model}} \times d_v}\)</span>分别是 query，key 和 value 的权重矩阵。</p> <p>我们将此时的 Attention 结果记为<span class=arithmatex>\(Y_n\)</span>，则有：</p> <div class=arithmatex>\[ Y_n = \text{softmax}(\text{Mask}(\frac{Q_nK_n^T}{\sqrt{d_k}}))V_n \]</div> <p>在生成第 <span class=arithmatex>\(n+1\)</span> 个 token 后，其作为新的输入 <span class=arithmatex>\(x_{n+1} \in \mathbb{R}^{1 \times d_{\text{model}}}\)</span>进入模型 (自回归生成)， 此时输入张量变为<span class=arithmatex>\(X_{n+1} \in \mathbb{R}^{(n+1) \times d_{\text{model}}}\)</span>:</p> <div class=arithmatex>\[ \begin{align} X_{n+1} = \left [ \begin{array}{c|c} X_{n} \\ x_{n+1} \\ \end{array} \right ] \end{align} \]</div> <p>此时为了计算下一个生成的 token，我们需要计算 Attention 结果 <span class=arithmatex>\(Y_{n+1}\)</span>：</p> <div class=arithmatex>\[ \begin{align} Y_{n+1} = \text{softmax}(\text{Mask}(\frac{Q_{n+1}K_{n+1}^T}{\sqrt{d_k}}))V_{n+1} \end{align} \]</div> <p>其中，</p> <div class=arithmatex>\[ \begin{array}{ccc} \begin{aligned} Q_{n+1} &amp;= X_{n+1} W^{Q} \\ &amp;= \left [ \begin{array}{c} X_{n} \\ \hline x_{n+1} \\ \end{array} \right ] W^{Q} \\ &amp;= \left [ \begin{array}{c} X_{n} W^{Q} \\ \hline x_{n+1} W^{Q} \\ \end{array} \right ] \\ &amp;= \left [ \begin{array}{c} Q_{n} \\ \hline q_{n+1} \\ \end{array} \right ] \end{aligned} &amp; \begin{aligned} K_{n+1} &amp;= X_{n+1} W^{K} \\ &amp;= \left [ \begin{array}{c} X_{n} \\ \hline x_{n+1} \\ \end{array} \right ] W^{K} \\ &amp;= \left [ \begin{array}{c} X_{n} W^{K} \\ \hline x_{n+1} W^{K} \\ \end{array} \right ] \\ &amp;= \left [ \begin{array}{c} K_{n} \\ \hline k_{n+1} \\ \end{array} \right ] \end{aligned} &amp; \begin{aligned} V_{n+1} &amp;= X_{n+1} W^{V} \\ &amp;= \left [ \begin{array}{c} X_{n} \\ \hline x_{n+1} \\ \end{array} \right ] W^{V} \\ &amp;= \left [ \begin{array}{c} X_{n} W^{V} \\ \hline x_{n+1} W^{V} \\ \end{array} \right ] \\ &amp;= \left [ \begin{array}{c} V_{n} \\ \hline v_{n+1} \\ \end{array} \right ] \end{aligned} \end{array} \]</div> <p><span class=arithmatex>\(Y_{n+1}\)</span>的计算过程如下：</p> <div class=arithmatex>\[ \begin{align} Y_{n+1} &amp;= \text{softmax} \left( \text{Mask} \left( \frac{ Q_{n+1} K_{n+1}^{\top}}{\sqrt{d_k}} \right) \right) V_{n+1} \\ &amp;= \text{softmax} \left( \text{Mask} \left( \frac{1}{\sqrt{d_k}} \left [ \begin{array}{c|c} Q_{n} \\ q_{n+1} \\ \end{array} \right ] \left [ \begin{array}{c|c} K_{n} \\ k_{n+1} \\ \end{array} \right ]^{\top} \right) \right) \left [ \begin{array}{c|c} V_{n} \\ v_{n+1} \\ \end{array} \right ] \\ &amp;= \text{softmax} \left( \text{Mask} \left( \frac{1}{\sqrt{d_k}} \left [ \begin{array}{c|c} Q_{n} \\ q_{n+1} \\ \end{array} \right ] \left [ \begin{array}{c|c} K_{n}^{\top} &amp; k_{n+1}^{\top} \\ \end{array} \right ] \right) \right) \left [ \begin{array}{c|c} V_{n} \\ v_{n+1} \\ \end{array} \right ] \\ &amp;= \text{softmax} \left( \text{Mask} \left( \frac{1}{\sqrt{d_k}} \left [ \begin{array}{c|c} Q_{n}K_{n}^{\top} &amp; Q_{n}k_{n+1}^{\top} \\ \hline q_{n+1}K_{n}^{\top} &amp; q_{n+1}k_{n+1}^{\top} \\ \end{array} \right ] \right) \right) \left [ \begin{array}{c|c} V_{n} \\ v_{n+1} \\ \end{array} \right ] \\ &amp;= \text{softmax} \left( \left [ \begin{array}{c|c} \text{Mask} \left( \frac{1}{\sqrt{d_k}} Q_{n}K_{n}^{\top}\right) &amp; -\infty \\ \hline \frac{1}{\sqrt{d_k}} q_{n+1}K_{n}^{\top} &amp; \frac{1}{\sqrt{d_k}} q_{n+1}k_{n+1}^{\top} \\ \end{array} \right ] \right) \left [ \begin{array}{c|c} V_{n} \\ v_{n+1} \\ \end{array} \right ] \\ &amp;= \left [ \begin{array}{c|c} \left [ \begin{array}{c|c} \text{softmax} \left(\text{Mask} \left( \frac{1}{\sqrt{d_k}} Q_{n}K_{n}^{\top}\right) \right) &amp; 0 \\ \end{array} \right ] \\ \hline \text{softmax} \left( \frac{1}{\sqrt{d_k}}q_{n+1} \left [ \begin{array}{c|c} K_{n}^{\top} &amp; k_{n+1}^{\top} \\ \end{array} \right ] \right) \\ \end{array} \right ] \left [ \begin{array}{c|c} V_{n} \\ v_{n+1} \\ \end{array} \right ] \\ &amp;= \left [ \begin{array}{c|c} \text{softmax} \left(\text{Mask} \left( \frac{1}{\sqrt{d_k}} Q_{n}K_{n}^{\top}\right) \right) V_{n} \\ \hline \text{softmax} \left( \frac{1}{\sqrt{d_k}}q_{n+1} K_{n+1}^{\top} \right) V_{n + 1} \\ \end{array} \right ] \\ &amp;= \left [ \begin{array}{c|c} Y_{n} \\ \text{softmax} \left( \frac{1}{\sqrt{d_k}}q_{n+1} K_{n+1}^{\top} \right) V_{n + 1} \\ \end{array} \right ] \\ &amp;= \left [ \begin{array}{c|c} Y_{n} \\ y_{n+1} \\ \end{array} \right ] \\ \end{align} \]</div> <p>从上面的推导我们可以看到，<span class=arithmatex>\(Y_{n+1}\)</span>可以分解为两部分：</p> <ol> <li>历史 token 的 attention 结果<span class=arithmatex>\(Y_n\)</span>，这部分在之前已经计算过</li> <li>新 token 的 attention 结果<span class=arithmatex>\(y_{n+1}\)</span>，这部分需要重新计算</li> </ol> <p>可以看到，在拿到第<span class=arithmatex>\(n+1\)</span>个 token 后，我们只需要计算这个新 token 的 attention 结果<span class=arithmatex>\(y_{n+1}\)</span>， 因为<span class=arithmatex>\(Y_n\)</span>已经计算过了，不需要重新计算。也就是说，在不使用 KV Cache 时，每次生成一个新 token 时，我们都需要：</p> <ol> <li>计算当前输入序列<span class=arithmatex>\(X_{n+1}\)</span>完整的 attention 矩阵<span class=arithmatex>\(Q_{n+1}K_{n+1}^T\)</span>，计算复杂度为<span class=arithmatex>\(O(n^2)\)</span></li> <li>对 <span class=arithmatex>\(n\)</span> 个 token 重复这个过程，总计算复杂度为<span class=arithmatex>\(O(n^3)\)</span></li> </ol> <p><strong>KV Cache 避免重复计算的方式是缓存数据和变更计算流程：缓存数据就是指缓存<span class=arithmatex>\(K_n\)</span>和<span class=arithmatex>\(V_n\)</span>，变更计算流程就是指在生成新 token 时，只需要计算新 token 的 query 与所有 key 的点积。前者避免了<span class=arithmatex>\(K_n\)</span>和<span class=arithmatex>\(V_n\)</span>的重复计算，后者避免了<span class=arithmatex>\(Y_n\)</span>的重复计算</strong>。</p> <figure> <img alt src=../../../../images/llm_kv_cache/attention_kv.png width=800> <figcaption>KV Cache 计算过程。图片来源：<cite>Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models</cite></figcaption> </figure> <p>所以在使用 KV Cache 后：</p> <ol> <li>计算 attention 矩阵时，只需要计算新 token 的 query 向量与所有 key 向量的点积，再乘以 value 向量，计算复杂度为<span class=arithmatex>\(O(n)\)</span></li> <li>对 <span class=arithmatex>\(n\)</span> 个 token 重复这个过程，总计算复杂度为<span class=arithmatex>\(O(n^2)\)</span></li> </ol> <p>如此一来，我们就将计算复杂度从<span class=arithmatex>\(O(n^3)\)</span>降低到了<span class=arithmatex>\(O(n^2)\)</span>，这里的<span class=arithmatex>\(n\)</span>是输入序列的长度，<span class=arithmatex>\(n\)</span>越大，推理加速效果越明显 (当然显存占用也会增加)。</p> <h2 id=code>Code<a class=headerlink href=#code title="Permanent link">¶</a></h2> <p>代码的实现其实也很简单，我们先来如何变更计算流程来使用 KV Cache。 首先是 <code>generate</code> 方法的变更，这里我们只展示关键的变更部分。 可以看到，这里每次传入的 <code>model_input_tokens</code> 是当前的输入序列，除了首次传入的长度为 <code>prompt_tokens.shape[1]</code> 的序列外， 之后每次传入的序列长度均为<code>cur_pos - prev_pos = 1</code>:</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal> 1</span>
<span class=normal> 2</span>
<span class=normal> 3</span>
<span class=normal> 4</span>
<span class=normal> 5</span>
<span class=normal> 6</span>
<span class=normal> 7</span>
<span class=normal> 8</span>
<span class=normal> 9</span>
<span class=normal>10</span>
<span class=normal>11</span>
<span class=normal>12</span>
<span class=normal>13</span>
<span class=normal>14</span>
<span class=normal>15</span>
<span class=normal>16</span>
<span class=normal>17</span>
<span class=normal>18</span>
<span class=normal>19</span>
<span class=normal>20</span>
<span class=normal>21</span>
<span class=normal>22</span>
<span class=normal>23</span>
<span class=normal>24</span>
<span class=normal>25</span>
<span class=normal>26</span>
<span class=normal>27</span>
<span class=normal>28</span></pre></div></td><td class=code><div><pre><span></span><code><span class=k>def</span> <span class=nf>generate</span><span class=p>(</span>
    <span class=bp>self</span><span class=p>,</span>
    <span class=n>prompt</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
    <span class=n>config</span><span class=p>:</span> <span class=n>GenerationConfig</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
<span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
    <span class=o>...</span>

    <span class=n>prompt_tokens</span> <span class=o>=</span> <span class=o>...</span>
<span class=hll>    <span class=n>prev_pos</span> <span class=o>=</span> <span class=mi>0</span>
</span><span class=hll>    <span class=k>for</span> <span class=n>cur_pos</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>prompt_tokens</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span>
</span>                         <span class=n>prompt_tokens</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>config</span><span class=o>.</span><span class=n>max_new_tokens</span><span class=p>,</span>
                         <span class=p>):</span>
<span class=hll>        <span class=n>model_input_tokens</span> <span class=o>=</span> <span class=n>prompt_tokens</span><span class=p>[:,</span> <span class=n>prev_pos</span><span class=p>:</span><span class=n>cur_pos</span><span class=p>]</span>
</span>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>inference_mode</span><span class=p>():</span>
            <span class=c1># shape: (batch_size, n_tokens, vocab_size)</span>
<span class=hll>            <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>gpt_model</span><span class=p>(</span><span class=n>model_input_tokens</span><span class=p>,</span> <span class=n>prev_pos</span><span class=p>)</span>
</span>        <span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>  <span class=c1># shape: (batch_size, vocab_size)</span>

        <span class=o>...</span>

        <span class=n>next_token_id</span> <span class=o>=</span> <span class=o>...</span>

        <span class=n>prompt_tokens</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>prompt_tokens</span><span class=p>,</span> <span class=n>next_token_id</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=c1># Update the previous position</span>
<span class=hll>        <span class=n>prev_pos</span> <span class=o>=</span> <span class=n>cur_pos</span>
</span>
    <span class=n>generate_text</span> <span class=o>=</span> <span class=n>token_ids_to_text</span><span class=p>(</span><span class=n>prompt_tokens</span><span class=p>)</span>
    <span class=k>return</span> <span class=n>generate_text</span>
</code></pre></div></td></tr></table></div> <p>之后我们再深入这里<code>self.gpt_model</code>的实现，这里我们只展示关键的变更部分。</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal> 1</span>
<span class=normal> 2</span>
<span class=normal> 3</span>
<span class=normal> 4</span>
<span class=normal> 5</span>
<span class=normal> 6</span>
<span class=normal> 7</span>
<span class=normal> 8</span>
<span class=normal> 9</span>
<span class=normal>10</span>
<span class=normal>11</span>
<span class=normal>12</span>
<span class=normal>13</span>
<span class=normal>14</span>
<span class=normal>15</span>
<span class=normal>16</span>
<span class=normal>17</span>
<span class=normal>18</span>
<span class=normal>19</span>
<span class=normal>20</span>
<span class=normal>21</span>
<span class=normal>22</span>
<span class=normal>23</span>
<span class=normal>24</span>
<span class=normal>25</span>
<span class=normal>26</span>
<span class=normal>27</span>
<span class=normal>28</span>
<span class=normal>29</span>
<span class=normal>30</span>
<span class=normal>31</span>
<span class=normal>32</span>
<span class=normal>33</span>
<span class=normal>34</span>
<span class=normal>35</span>
<span class=normal>36</span></pre></div></td><td class=code><div><pre><span></span><code><span class=k>class</span> <span class=nc>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
        <span class=o>...</span>
    <span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=o>...</span>
<span class=hll>        <span class=bp>self</span><span class=o>.</span><span class=n>use_kv_cache</span> <span class=o>=</span> <span class=n>use_kv_cache</span>
</span><span class=hll>        <span class=k>if</span> <span class=n>use_kv_cache</span><span class=p>:</span>
</span><span class=hll>            <span class=bp>self</span><span class=o>.</span><span class=n>kv_cache</span> <span class=o>=</span> <span class=n>KVCache</span><span class=p>(</span>
</span><span class=hll>                <span class=n>batch_size</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span><span class=hll>                <span class=n>max_seq_len</span><span class=o>=</span><span class=n>ctx_len</span><span class=p>,</span>
</span><span class=hll>                <span class=n>num_kv_heads</span><span class=o>=</span><span class=n>n_heads</span><span class=p>,</span>
</span><span class=hll>                <span class=n>head_dim</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>,</span>
</span><span class=hll>                <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span>
</span><span class=hll>            <span class=p>)</span>
</span>
    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>:</span> <span class=n>GPTInnerType</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>GPTInnerType</span><span class=p>:</span>
        <span class=n>batch_size</span><span class=p>,</span> <span class=n>num_tokens</span><span class=p>,</span> <span class=n>_d_in</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>

        <span class=n>queries</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_query</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>keys</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_key</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>values</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_value</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

        <span class=n>queries</span> <span class=o>=</span> <span class=n>queries</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>num_tokens</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>n_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
        <span class=n>keys</span> <span class=o>=</span> <span class=n>keys</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>num_tokens</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>n_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
        <span class=n>values</span> <span class=o>=</span> <span class=n>values</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>num_tokens</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>n_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>

        <span class=n>queries</span> <span class=o>=</span> <span class=n>queries</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
        <span class=n>keys</span> <span class=o>=</span> <span class=n>keys</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
        <span class=n>values</span> <span class=o>=</span> <span class=n>values</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>

<span class=hll>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_kv_cache</span><span class=p>:</span>
</span><span class=hll>            <span class=bp>self</span><span class=o>.</span><span class=n>kv_cache</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>keys</span><span class=p>,</span> <span class=n>values</span><span class=p>)</span>
</span><span class=hll>            <span class=n>keys</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>kv_cache</span><span class=o>.</span><span class=n>k_cache</span><span class=p>[:,</span> <span class=p>:,</span> <span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>kv_cache</span><span class=o>.</span><span class=n>size</span><span class=p>,</span> <span class=p>:]</span>
</span><span class=hll>            <span class=n>values</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>kv_cache</span><span class=o>.</span><span class=n>v_cache</span><span class=p>[:,</span> <span class=p>:,</span> <span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>kv_cache</span><span class=o>.</span><span class=n>size</span><span class=p>,</span> <span class=p>:]</span>
</span>        <span class=o>...</span>
</code></pre></div></td></tr></table></div> <p>最后我们来看 <code>KVCache</code> 的具体实现：</p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>


<span class=k>class</span> <span class=nc>KVCache</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>batch_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
        <span class=n>max_seq_len</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
        <span class=n>num_kv_heads</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
        <span class=n>head_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
        <span class=n>dtype</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>dtype</span><span class=p>,</span>
    <span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=n>cache_shape</span> <span class=o>=</span> <span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>num_kv_heads</span><span class=p>,</span> <span class=n>max_seq_len</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s2>"k_cache"</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>cache_shape</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>),</span> <span class=n>persistent</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s2>"v_cache"</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>cache_shape</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>),</span> <span class=n>persistent</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>batch_size</span> <span class=o>=</span> <span class=n>batch_size</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>cache_pos</span> <span class=o>=</span> <span class=mi>0</span>

    <span class=k>def</span> <span class=nf>reset</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
<span class=w>        </span><span class=sd>"""Reset the cache to zero."""</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>k_cache</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>v_cache</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>cache_pos</span> <span class=o>=</span> <span class=mi>0</span>

    <span class=nd>@property</span>
    <span class=k>def</span> <span class=nf>size</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_pos</span>

    <span class=k>def</span> <span class=nf>update</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>k_val</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>v_val</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]:</span>
<span class=w>        </span><span class=sd>"""Update KV cache with the new ``k_val``, ``v_val`` and return the updated cache.</span>

<span class=sd>        Args:</span>
<span class=sd>            k_val (torch.Tensor): Current key tensor with shape [B, H, S, D]</span>
<span class=sd>            v_val (torch.Tensor): Current value tensor with shape [B, H, S, D]</span>
<span class=sd>        """</span>
        <span class=n>bsz</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>k_val</span><span class=o>.</span><span class=n>shape</span>
        <span class=k>if</span> <span class=n>bsz</span> <span class=o>&gt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>k_cache</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]:</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>  <span class=c1># noqa: TRY003</span>
                <span class=sa>f</span><span class=s2>"The current cache has been setup with a batch size of </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>k_cache</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=si>}</span><span class=s2>"</span>  <span class=c1># noqa: EM102</span>
                <span class=sa>f</span><span class=s2>", but found new key tensors with batch size </span><span class=si>{</span><span class=n>k_val</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=si>}</span><span class=s2>!"</span>
            <span class=p>)</span>
        <span class=k>assert</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>cache_pos</span> <span class=o>+</span> <span class=n>seq_len</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=bp>self</span><span class=o>.</span><span class=n>k_cache</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span>  <span class=c1># noqa: S101</span>
        <span class=n>k_out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>k_cache</span>
        <span class=n>v_out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>v_cache</span>
        <span class=n>k_out</span><span class=p>[:,</span> <span class=p>:,</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_pos</span> <span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_pos</span> <span class=o>+</span> <span class=n>seq_len</span><span class=p>]</span> <span class=o>=</span> <span class=n>k_val</span>
        <span class=n>v_out</span><span class=p>[:,</span> <span class=p>:,</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_pos</span> <span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_pos</span> <span class=o>+</span> <span class=n>seq_len</span><span class=p>]</span> <span class=o>=</span> <span class=n>v_val</span>
        <span class=c1># forward cache_pos seq_len positions along</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>cache_pos</span> <span class=o>+=</span> <span class=n>seq_len</span>
        <span class=k>return</span> <span class=n>k_out</span><span class=p>,</span> <span class=n>v_out</span>
</code></pre></div> <details class=note> <summary>如果你对这里的<code>register_buffer</code>有疑惑</summary> <p>简单来说，<code>register_buffer</code>可以使得我们在模型中注册一个持久化的buffer，这个buffer不会被视为模型的参数(自然也不会更新)。 这里存在一个问题，那就是为什么要用<code>register_buffer</code>而不是直接用<code>self.k_cache = ...</code>呢？ 答案很简单，通过<code>register_buffer</code>注册的buffer可以随着<code>model.to(device)</code>的调用而自动转移到指定的设备上，而后一种方式则不会。</p> <blockquote> <p>In essence, PyTorch buffers are tensor attributes associated with a PyTorch module or model similar to parameters, but unlike parameters, buffers are not updated during training.</p> <p>Buffers in PyTorch are particularly useful when dealing with GPU computations, as they need to be transferred between devices (like from CPU to GPU) alongside the model's parameters. Unlike parameters, buffers do not require gradient computation, but they still need to be on the correct device to ensure that all computations are performed correctly.</p> </blockquote> <p>更多的细节可以参考以下链接：</p> <ul> <li> <p><a href=https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer>PyTorch register_buffer</a></p> </li> <li> <p><a href=https://github.com/rasbt/LLMs-from-scratch/blob/main/ch03/03_understanding-buffers/understanding-buffers.ipynb>LLMs-from-scratch explain</a></p> </li> </ul> </details> <p>注意这里<code>KVCache</code>内部<code>cache</code>的维度：<code>cache_shape = (batch_size, num_kv_heads, max_seq_len, head_dim)</code>. 其中，<code>batch_size</code>是模型的 batch size，<code>num_kv_heads</code>是模型的 kv head 的数量，<code>max_seq_len</code>是模型的最大序列长度，<code>head_dim</code>是每个 kv head 的维度。 也就是说，这里会缓存<strong>batch 中每个样本的每个 kv head 在每个位置上的 key 和 value(两者均为维度等于<code>head_dim</code>的向量)</strong>。</p> <p><code>KVCache</code>的<code>update</code>方法会将当前的<code>k_val</code>和<code>v_val</code>更新到缓存中，并返回更新后的缓存。 更新时的操作也很简单，就是把当前的<code>k_val</code>和<code>v_val</code>更新到缓存中对应的位置上：</p> <div class=highlight><pre><span></span><code><span class=n>k_out</span><span class=p>[:,</span> <span class=p>:,</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_pos</span> <span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_pos</span> <span class=o>+</span> <span class=n>seq_len</span><span class=p>]</span> <span class=o>=</span> <span class=n>k_val</span>
<span class=n>v_out</span><span class=p>[:,</span> <span class=p>:,</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_pos</span> <span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_pos</span> <span class=o>+</span> <span class=n>seq_len</span><span class=p>]</span> <span class=o>=</span> <span class=n>v_val</span>
<span class=bp>self</span><span class=o>.</span><span class=n>cache_pos</span> <span class=o>+=</span> <span class=n>seq_len</span>
</code></pre></div> <p>这里的<code>self.cache_pos</code>表示当前缓存的最后一个位置，<code>seq_len</code>表示当前输入的序列长度。</p> <details class=note open=open> <summary>和 torchtune 实现的些许差别</summary> <p>如果你之前看过torchtune的KV Cache实现，那么你会发现这里KV Cache的实现和torchtune基本是一样的——除了<code>cache_pos</code>的实现。 在torchtune最早的实现中<code>cache_pos</code>就是上面的这种形式，不过后续为了兼容<code>torch.compile</code>将其实现为一个向量而不是一个整数。 具体参考对应issue: <a href=https://github.com/pytorch/torchtune/issues/2564>#2564</a>, <a href=https://github.com/pytorch/torchtune/issues/1663>#1663</a>.</p> </details> <h2 id=the-end>The end<a class=headerlink href=#the-end title="Permanent link">¶</a></h2> <p>KV Cache 作为一项核心的 LLM 推理优化技术，已经在很多框架中应用，相关优化也在持续进行中。本文难以详尽介绍，故仅从其原始形态管窥一二。</p> <p>最后，笔者花费了大量时间构思本文，力求简明易懂，但仍未达到理想状态。因拖延已久，决定先行发布，后续有时间再补充修改。</p></div> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title=最后更新> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">2025年6月3日</span> </span> <span class=md-source-file__fact> <span class=md-icon title=创建日期> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">2025年6月3日</span> </span> </aside> <script src=https://giscus.app/client.js data-repo=shenxiangzhuang/shenxiangzhuang.github.io data-repo-id="MDEwOlJlcG9zaXRvcnk4MjU0MjM1OQ==" data-category=Announcements data-category-id=DIC_kwDOBOt_F84CTHBH data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async>
</script> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> 回到页面顶部 </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2017 - 2024 Xiangzhuang Shen </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://datahonor.com/feed_rss_created target=_blank rel=noopener title=datahonor.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M6.18 15.64a2.18 2.18 0 0 1 2.18 2.18C8.36 19 7.38 20 6.18 20 5 20 4 19 4 17.82a2.18 2.18 0 0 1 2.18-2.18M4 4.44A15.56 15.56 0 0 1 19.56 20h-2.83A12.73 12.73 0 0 0 4 7.27zm0 5.66a9.9 9.9 0 0 1 9.9 9.9h-2.83A7.07 7.07 0 0 0 4 12.93z"/></svg> </a> <a href=https://github.com/shenxiangzhuang target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://twitter.com/MathewShen42 target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9L389.2 48zm-24.8 373.8h39.1L151.1 88h-42l255.3 333.8z"/></svg> </a> <a href=https://linkedin.com/in/mathewshen target=_blank rel=noopener title=linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> <a href=mailto:datahonor@gmail.com target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2"/></svg> </a> <a href=https://mp.weixin.qq.com/s/wOqp6nHBAenK9wP2vIUS9g target=_blank rel=noopener title=mp.weixin.qq.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.691 2.188C3.891 2.188 0 5.476 0 9.53c0 2.212 1.17 4.203 3.002 5.55a.59.59 0 0 1 .213.665l-.39 1.48c-.019.07-.048.141-.048.213 0 .163.13.295.29.295a.33.33 0 0 0 .167-.054l1.903-1.114a.86.86 0 0 1 .717-.098 10.2 10.2 0 0 0 2.837.403c.276 0 .543-.027.811-.05-.857-2.578.157-4.972 1.932-6.446 1.703-1.415 3.882-1.98 5.853-1.838-.576-3.583-4.196-6.348-8.596-6.348M5.785 5.991c.642 0 1.162.529 1.162 1.18a1.17 1.17 0 0 1-1.162 1.178A1.17 1.17 0 0 1 4.623 7.17c0-.651.52-1.18 1.162-1.18zm5.813 0c.642 0 1.162.529 1.162 1.18a1.17 1.17 0 0 1-1.162 1.178 1.17 1.17 0 0 1-1.162-1.178c0-.651.52-1.18 1.162-1.18m5.34 2.867c-1.797-.052-3.746.512-5.28 1.786-1.72 1.428-2.687 3.72-1.78 6.22.942 2.453 3.666 4.229 6.884 4.229.826 0 1.622-.12 2.361-.336a.72.72 0 0 1 .598.082l1.584.926a.3.3 0 0 0 .14.047c.134 0 .24-.111.24-.247 0-.06-.023-.12-.038-.177l-.327-1.233a.6.6 0 0 1-.023-.156.49.49 0 0 1 .201-.398C23.024 18.48 24 16.82 24 14.98c0-3.21-2.931-5.837-6.656-6.088V8.89c-.135-.01-.27-.027-.407-.03zm-2.53 3.274c.535 0 .969.44.969.982a.976.976 0 0 1-.969.983.976.976 0 0 1-.969-.983c0-.542.434-.982.97-.982zm4.844 0c.535 0 .969.44.969.982a.976.976 0 0 1-.969.983.976.976 0 0 1-.969-.983c0-.542.434-.982.969-.982"/></svg> </a> <a href=https://www.zhihu.com/people/shen-xiang-zhuang target=_blank rel=noopener title=www.zhihu.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M5.721 0C2.251 0 0 2.25 0 5.719V18.28C0 21.751 2.252 24 5.721 24h12.56C21.751 24 24 21.75 24 18.281V5.72C24 2.249 21.75 0 18.281 0zm1.964 4.078q-.408 1.096-.68 2.11h4.587c.545-.006.445 1.168.445 1.171H9.384a58 58 0 0 1-.112 3.797h2.712c.388.023.393 1.251.393 1.266H9.183a9.2 9.2 0 0 1-.408 2.102l.757-.604c.452.456 1.512 1.712 1.906 2.177.473.681.063 2.081.063 2.081l-2.794-3.382c-.653 2.518-1.845 3.607-1.845 3.607-.523.468-1.58.82-2.64.516 2.218-1.73 3.44-3.917 3.667-6.497H4.491c0-.015.197-1.243.806-1.266h2.71c.024-.32.086-3.254.086-3.797H6.598c-.136.406-.158.447-.268.753-.594 1.095-1.603 1.122-1.907 1.155.906-1.821 1.416-3.6 1.591-4.064.425-1.124 1.671-1.125 1.671-1.125M13.078 6h6.377v11.33h-2.573l-2.184 1.373-.401-1.373h-1.219zm1.313 1.219v8.86h.623l.263.937 1.455-.938h1.456v-8.86z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-consent data-md-component=consent id=__consent hidden> <div class=md-consent__overlay></div> <aside class=md-consent__inner> <form class="md-consent__form md-grid md-typeset" name=consent> <h4>Cookie consent</h4> <p>We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.</p> <input class=md-toggle type=checkbox id=__settings> <div class=md-consent__settings> <ul class=task-list> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=analytics checked> <span class=task-list-indicator></span> Google Analytics </label> </li> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=github checked> <span class=task-list-indicator></span> GitHub </label> </li> </ul> </div> <div class=md-consent__controls> <button class="md-button md-button--primary">同意</button> <label class=md-button for=__settings>管理设定</label> </div> </form> </aside> </div> <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script> <script id=__config type=application/json>{"base": "../../../../..", "features": ["content.action.edit", "content.action.view", "announce.dismiss", "content.code.annotate", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.indexes", "navigation.prune", "navigation.sections", "navigation.top", "search.highlight", "search.share", "search.suggest", "toc.follow", "content.code.annotate"], "search": "../../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script> <script src=../../../../../assets/javascripts/bundle.83f73b43.min.js></script> <script src=../../../../../javascripts/mathjax.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>