<!doctype html><html lang=zh class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="记录 BERT 论文的复现过程与经验教训。这是我一直想要找的一篇博客，但我一直没找到，于是决定自己来写一篇。
"><meta name=author content="Xiangzhuang Shen"><link href=https://shenxiangzhuang.github.io/blog/2025/11/02/bert/ rel=canonical><link href=../../../10/16/open_source_feedback/ rel=prev><link href=../../23/toynlp/ rel=next><link rel=alternate type=application/rss+xml title="RSS 订阅" href=../../../../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="已更新内容的 RSS 订阅" href=../../../../../feed_rss_updated.xml><link rel=icon href=../../../../../assets/logo/glimpse-ai-logo-small.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.5.42"><title>BERT 论文复现 - Data Honor</title><link rel=stylesheet href=../../../../../assets/stylesheets/main.0253249f.min.css><link rel=stylesheet href=../../../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../../../css/neoteroi-mkdocs.css><script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-KEEHE7VY5K"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-KEEHE7VY5K",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-KEEHE7VY5K",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>if("undefined"!=typeof __md_analytics){var consent=__md_get("__consent");consent&&consent.analytics&&__md_analytics()}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=teal data-md-color-accent=deep-purple> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#_1 class=md-skip> 跳转至 </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label=不再显示此消息> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> <center> You can follow by <a href=https://datahonor.com/feed_rss_created> <strong>RSS</strong> </a> or <a href=https://github.com/shenxiangzhuang> <span class=twemoji> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M448 96c0-35.3-28.7-64-64-64H64C28.7 32 0 60.7 0 96v320c0 35.3 28.7 64 64 64h320c35.3 0 64-28.7 64-64V96zM265.8 407.7c0-1.8 0-6 .1-11.6.1-11.4.1-28.8.1-43.7 0-15.6-5.2-25.5-11.3-30.7 37-4.1 76-9.2 76-73.1 0-18.2-6.5-27.3-17.1-39 1.7-4.3 7.4-22-1.7-45-13.9-4.3-45.7 17.9-45.7 17.9-13.2-3.7-27.5-5.6-41.6-5.6s-28.4 1.9-41.6 5.6c0 0-31.8-22.2-45.7-17.9-9.1 22.9-3.5 40.6-1.7 45-10.6 11.7-15.6 20.8-15.6 39 0 63.6 37.3 69 74.3 73.1-4.8 4.3-9.1 11.7-10.6 22.3-9.5 4.3-33.8 11.7-48.3-13.9-9.1-15.8-25.5-17.1-25.5-17.1-16.2-.2-1.1 10.2-1.1 10.2 10.8 5 18.4 24.2 18.4 24.2 9.7 29.7 56.1 19.7 56.1 19.7 0 9 .1 21.7.1 30.6 0 4.8.1 8.6.1 10 0 4.3-3 9.5-11.5 8-66-22.1-112.2-84.9-112.2-158.3 0-91.8 70.2-161.5 162-161.5S388 165.6 388 257.4c.1 73.4-44.7 136.3-110.7 158.3-8.4 1.5-11.5-3.7-11.5-8zm-90.5-54.8c-.2-1.5 1.1-2.8 3-3.2 1.9-.2 3.7.6 3.9 1.9.3 1.3-1 2.6-3 3-1.9.4-3.7-.4-3.9-1.7zm-9.1 3.2c-2.2.2-3.7-.9-3.7-2.4 0-1.3 1.5-2.4 3.5-2.4 1.9-.2 3.7.9 3.7 2.4 0 1.3-1.5 2.4-3.5 2.4zm-14.3-2.2c-1.9-.4-3.2-1.9-2.8-3.2s2.4-1.9 4.1-1.5c2 .6 3.3 2.1 2.8 3.4-.4 1.3-2.4 1.9-4.1 1.3zm-12.5-7.3c-1.5-1.3-1.9-3.2-.9-4.1.9-1.1 2.8-.9 4.3.6 1.3 1.3 1.8 3.3.9 4.1-.9 1.1-2.8.9-4.3-.6zm-8.5-10c-1.1-1.5-1.1-3.2 0-3.9 1.1-.9 2.8-.2 3.7 1.3 1.1 1.5 1.1 3.3 0 4.1-.9.6-2.6 0-3.7-1.5zm-6.3-8.8c-1.1-1.3-1.3-2.8-.4-3.5.9-.9 2.4-.4 3.5.6 1.1 1.3 1.3 2.8.4 3.5-.9.9-2.4.4-3.5-.6zm-6-6.4c-1.3-.6-1.9-1.7-1.5-2.6.4-.6 1.5-.9 2.8-.4 1.3.7 1.9 1.8 1.5 2.6-.4.9-1.7 1.1-2.8.4z"/></svg> </span> <strong>GitHub(<strong>shenxiangzhuang</strong>)</strong> </a> <!--  or--> <!--  <a href="https://www.zhihu.com/people/shen-xiang-zhuang">--> <!--    <strong>知乎</strong>--> <!--  </a>--> or <!--暂时用 LLM in 2024 文章链接作为入口--> <a href=https://mp.weixin.qq.com/s/wOqp6nHBAenK9wP2vIUS9g> <span class=twemoji> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 4C5.36 4 2 6.69 2 10c0 1.89 1.08 3.56 2.78 4.66L4 17l2.5-1.5c.89.31 1.87.5 2.91.5A5.22 5.22 0 0 1 9 14c0-3.31 3.13-6 7-6 .19 0 .38 0 .56.03C15.54 5.69 12.78 4 9.5 4m-3 2.5a1 1 0 0 1 1 1 1 1 0 0 1-1 1 1 1 0 0 1-1-1 1 1 0 0 1 1-1m5 0a1 1 0 0 1 1 1 1 1 0 0 1-1 1 1 1 0 0 1-1-1 1 1 0 0 1 1-1M16 9c-3.31 0-6 2.24-6 5s2.69 5 6 5c.67 0 1.31-.08 1.91-.25L20 20l-.62-1.87C20.95 17.22 22 15.71 22 14c0-2.76-2.69-5-6-5m-2 2.5a1 1 0 0 1 1 1 1 1 0 0 1-1 1 1 1 0 0 1-1-1 1 1 0 0 1 1-1m4 0a1 1 0 0 1 1 1 1 1 0 0 1-1 1 1 1 0 0 1-1-1 1 1 0 0 1 1-1Z"/></svg> </span> <strong>WeChat(<strong>AI Glimpse</strong>)</strong> </a> </center> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=页眉> <a href=../../../../.. title="Data Honor" class="md-header__button md-logo" aria-label="Data Honor" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M6.27 17.05A2.991 2.991 0 0 1 4 22c-1.66 0-3-1.34-3-3s1.34-3 3-3c.18 0 .36 0 .53.05l3.07-5.36-1.74-.99 4.09-1.12 1.12 4.09-1.74-.99zM20 16c-1.3 0-2.4.84-2.82 2H11v-2l-3 3 3 3v-2h6.18c.42 1.16 1.52 2 2.82 2 1.66 0 3-1.34 3-3s-1.34-3-3-3m-8-8c.18 0 .36 0 .53-.05l3.07 5.36-1.74.99 4.09 1.12 1.12-4.09-1.74.99-3.06-5.37A2.991 2.991 0 0 0 12 2c-1.66 0-3 1.34-3 3s1.34 3 3 3"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Data Honor </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> BERT 论文复现 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=teal data-md-color-accent=deep-purple aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=cyan data-md-color-accent=deep-purple aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=搜索 placeholder=搜索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=查找> <a href=javascript:void(0) class="md-search__icon md-icon" title=分享 aria-label=分享 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=清空当前内容 aria-label=清空当前内容 tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/shenxiangzhuang/shenxiangzhuang.github.io title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> Xiangzhuang Shen's Blog </div> </a> </div> </nav> <nav class=md-tabs aria-label=标签 data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../../.. class=md-tabs__link> About </a> </li> <li class=md-tabs__item> <a href=../../../../../datascience/statistics/ class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../../../../../cs/programming/ class=md-tabs__link> Computer Science </a> </li> <li class=md-tabs__item> <a href=../../../../../se/ class=md-tabs__link> Software Engineering </a> </li> <li class=md-tabs__item> <a href=../../../../../odyssey/ class=md-tabs__link> Career Odyssey </a> </li> <li class=md-tabs__item> <a href=../../../../../tool/ class=md-tabs__link> Tool </a> </li> <li class=md-tabs__item> <a href=../../../../../project/ class=md-tabs__link> Project </a> </li> <li class=md-tabs__item> <a href=../../../../../life/ class=md-tabs__link> Life </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../../ class=md-tabs__link> Blog </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=导航栏 data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../../.. title="Data Honor" class="md-nav__button md-logo" aria-label="Data Honor" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M6.27 17.05A2.991 2.991 0 0 1 4 22c-1.66 0-3-1.34-3-3s1.34-3 3-3c.18 0 .36 0 .53.05l3.07-5.36-1.74-.99 4.09-1.12 1.12 4.09-1.74-.99zM20 16c-1.3 0-2.4.84-2.82 2H11v-2l-3 3 3 3v-2h6.18c.42 1.16 1.52 2 2.82 2 1.66 0 3-1.34 3-3s-1.34-3-3-3m-8-8c.18 0 .36 0 .53-.05l3.07 5.36-1.74.99 4.09 1.12 1.12-4.09-1.74.99-3.06-5.37A2.991 2.991 0 0 0 12 2c-1.66 0-3 1.34-3 3s1.34 3 3 3"/></svg> </a> Data Honor </label> <div class=md-nav__source> <a href=https://github.com/shenxiangzhuang/shenxiangzhuang.github.io title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> Xiangzhuang Shen's Blog </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../.. class=md-nav__link> <span class=md-ellipsis> About </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../datascience/statistics/ class=md-nav__link> <span class=md-ellipsis> Data Science </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../cs/programming/ class=md-nav__link> <span class=md-ellipsis> Computer Science </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../se/ class=md-nav__link> <span class=md-ellipsis> Software Engineering </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../odyssey/ class=md-nav__link> <span class=md-ellipsis> Career Odyssey </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../tool/ class=md-nav__link> <span class=md-ellipsis> Tool </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../project/ class=md-nav__link> <span class=md-ellipsis> Project </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../life/ class=md-nav__link> <span class=md-ellipsis> Life </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9 checked> <div class="md-nav__link md-nav__container"> <a href=../../../../ class="md-nav__link "> <span class=md-ellipsis> Blog </span> </a> <label class="md-nav__link " for=__nav_9 id=__nav_9_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_9_label aria-expanded=true> <label class=md-nav__title for=__nav_9> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9_2> <label class=md-nav__link for=__nav_9_2 id=__nav_9_2_label tabindex> <span class=md-ellipsis> 归档 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_9_2_label aria-expanded=false> <label class=md-nav__title for=__nav_9_2> <span class="md-nav__icon md-icon"></span> 归档 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../archive/2025/ class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2022/ class=md-nav__link> <span class=md-ellipsis> 2022 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2021/ class=md-nav__link> <span class=md-ellipsis> 2021 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2019/ class=md-nav__link> <span class=md-ellipsis> 2019 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2018/ class=md-nav__link> <span class=md-ellipsis> 2018 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_9_3> <label class=md-nav__link for=__nav_9_3 id=__nav_9_3_label tabindex> <span class=md-ellipsis> 分类 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_9_3_label aria-expanded=false> <label class=md-nav__title for=__nav_9_3> <span class="md-nav__icon md-icon"></span> 分类 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../category/ai/ class=md-nav__link> <span class=md-ellipsis> AI </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/engineering/ class=md-nav__link> <span class=md-ellipsis> Engineering </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/llm/ class=md-nav__link> <span class=md-ellipsis> LLM </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/mle/ class=md-nav__link> <span class=md-ellipsis> MLE </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/nlp/ class=md-nav__link> <span class=md-ellipsis> NLP </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/odyssey/ class=md-nav__link> <span class=md-ellipsis> Odyssey </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/paper/ class=md-nav__link> <span class=md-ellipsis> Paper </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/project/ class=md-nav__link> <span class=md-ellipsis> Project </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/statistics/ class=md-nav__link> <span class=md-ellipsis> Statistics </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E4%B9%A6%E8%AF%84/ class=md-nav__link> <span class=md-ellipsis> 书评 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E5%9B%9E%E5%BF%86%E5%BD%95/ class=md-nav__link> <span class=md-ellipsis> 回忆录 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E5%BC%80%E6%BA%90/ class=md-nav__link> <span class=md-ellipsis> 开源 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/ class=md-nav__link> <span class=md-ellipsis> 数据分析 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E6%96%87%E5%AD%A6/ class=md-nav__link> <span class=md-ellipsis> 文学 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E6%97%A5%E8%AE%B0/ class=md-nav__link> <span class=md-ellipsis> 日记 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E7%AE%97%E6%B3%95/ class=md-nav__link> <span class=md-ellipsis> 算法 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E7%BB%88%E8%BA%AB%E5%AD%A6%E4%B9%A0/ class=md-nav__link> <span class=md-ellipsis> 终身学习 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/%E9%9A%8F%E7%AC%94/ class=md-nav__link> <span class=md-ellipsis> 随笔 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 关于我们做到了哪一步 </span> </a> </li> <li class=md-nav__item> <a href=#bert class=md-nav__link> <span class=md-ellipsis> BERT 简介 </span> </a> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 有趣的观察 </span> </a> <nav class=md-nav aria-label=有趣的观察> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_3 class=md-nav__link> <span class=md-ellipsis> 预训练过程的有趣现象 </span> </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> <span class=md-ellipsis> 数据非常非常重要 </span> </a> </li> <li class=md-nav__item> <a href=#finetune class=md-nav__link> <span class=md-ellipsis> 从 Finetune 管窥预训练的威力 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#_5 class=md-nav__link> <span class=md-ellipsis> 错误与教训 </span> </a> <nav class=md-nav aria-label=错误与教训> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_6 class=md-nav__link> <span class=md-ellipsis> 即使在极小数据集上也无法过拟合 </span> </a> </li> <li class=md-nav__item> <a href=#cpu-gpu class=md-nav__link> <span class=md-ellipsis> 一次性预处理全量数据，CPU 加班，GPU 休假 </span> </a> </li> <li class=md-nav__item> <a href=#_7 class=md-nav__link> <span class=md-ellipsis> 试图用临时手搓的玩意儿达到很好的性能 </span> </a> </li> <li class=md-nav__item> <a href=#workers-prefetch-oom class=md-nav__link> <span class=md-ellipsis> workers 和 prefetch 因子设得过大导致 OOM </span> </a> </li> <li class=md-nav__item> <a href=#dataloader-worker class=md-nav__link> <span class=md-ellipsis> 没有优雅处理 DataLoader worker 的异常 </span> </a> </li> <li class=md-nav__item> <a href=#tokenizer class=md-nav__link> <span class=md-ellipsis> 评估时使用了错误的 tokenizer </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#_8 class=md-nav__link> <span class=md-ellipsis> 最后 </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-content md-content--post" data-md-component=content> <div class="md-sidebar md-sidebar--post" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class="md-sidebar__inner md-post"> <nav class="md-nav md-nav--primary"> <div class=md-post__back> <div class="md-nav__title md-nav__container"> <a href=../../../../ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> <span class=md-ellipsis> 回到主页 </span> </a> </div> </div> <div class="md-post__authors md-typeset"> <div class="md-profile md-post__profile"> <span class="md-author md-author--long"> <img src=https://avatars.githubusercontent.com/u/17157965 alt="Xiangzhuang Shen"> </span> <span class=md-profile__description> <strong> <a href=https://github.com/shenxiangzhuang>Xiangzhuang Shen</a> </strong> <br> Creator </span> </div> </div> <ul class="md-post__meta md-nav__list"> <li class="md-nav__item md-nav__item--section"> <div class=md-post__title> <span class=md-ellipsis> 元数据 </span> </div> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg> <time datetime="2025-11-02 00:00:00" class=md-ellipsis>2025年11月2日</time> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg> <span class=md-ellipsis> 分类于 <a href=../../../../category/%E7%BB%88%E8%BA%AB%E5%AD%A6%E4%B9%A0/ >终身学习</a>, <a href=../../../../category/nlp/ >NLP</a></span> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg> <span class=md-ellipsis> 需要 8 分钟阅读时间 </span> </div> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <article class="md-content__inner md-typeset"> <a href=https://github.com/shenxiangzhuang/shenxiangzhuang.github.io/edit/master/docs/blog/posts/bert.md title=编辑此页 class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/shenxiangzhuang/shenxiangzhuang.github.io/raw/master/docs/blog/posts/bert.md title=查看本页的源代码 class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <h1>BERT 论文复现</h1> <div><p><img alt src=../../../../images/bert/no_fear_future.png></p> <h2 id=_1>关于我们做到了哪一步<a class=headerlink href=#_1 title="Permanent link">¶</a></h2> <p>我在网上搜到了很多 BERT 复现的项目，有博客，也有 GitHub 代码仓库等。 这些文章和代码会告诉你他们在复现 BERT，介绍 BERT 是什么，原理是什么，代码要怎么写…… 但是几乎没有人提及是否复现了论文的指标，甚至也不会放当前实现的指标出来——这就让人不得不怀疑其正确性。所以在开始之前，我们先来谈一谈正确性的问题。</p> <!-- more --> <p>首先是模型结构的正确性。我们通过和 Huggingface 上的 BERT 实现进行对比，确保了模型参数是完全一致的，即 base-bert 模型参数为 109482240，large-bert 模型参数为 335141888：</p> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>test_bert_architecture</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
    <span class=c1># base bert config</span>
    <span class=n>base_bert_config</span> <span class=o>=</span> <span class=n>BertConfig</span><span class=p>(</span>
        <span class=n>max_seq_length</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span>
        <span class=n>vocab_size</span><span class=o>=</span><span class=mi>30522</span><span class=p>,</span>
        <span class=n>d_model</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span>
        <span class=n>attention_d_k</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span>
        <span class=n>attention_d_v</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span>
        <span class=n>head_num</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span>
        <span class=n>d_feed_forward</span><span class=o>=</span><span class=mi>3072</span><span class=p>,</span>
        <span class=n>encoder_layers</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span>
    <span class=p>)</span>
    <span class=n>base_bert</span> <span class=o>=</span> <span class=n>Bert</span><span class=p>(</span><span class=n>base_bert_config</span><span class=p>,</span> <span class=n>padding_idx</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
    <span class=n>base_bert_param_count</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>base_bert</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>
    <span class=c1># Make sure the parameter count matches the expected value for BERT base</span>
    <span class=c1># 109482240 match EXACTLY with Huggingface implementation: BertModel.from_pretrained("bert-base-uncased")</span>
    <span class=k>assert</span> <span class=n>base_bert_param_count</span> <span class=o>==</span> <span class=mi>109482240</span>  <span class=c1># 110M params</span>

    <span class=c1># large bert config</span>
    <span class=n>large_bert_config</span> <span class=o>=</span> <span class=n>BertConfig</span><span class=p>(</span>
        <span class=n>max_seq_length</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span>
        <span class=n>vocab_size</span><span class=o>=</span><span class=mi>30522</span><span class=p>,</span>
        <span class=n>d_model</span><span class=o>=</span><span class=mi>1024</span><span class=p>,</span>
        <span class=n>attention_d_k</span><span class=o>=</span><span class=mi>1024</span><span class=p>,</span>
        <span class=n>attention_d_v</span><span class=o>=</span><span class=mi>1024</span><span class=p>,</span>
        <span class=n>head_num</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
        <span class=n>d_feed_forward</span><span class=o>=</span><span class=mi>4096</span><span class=p>,</span>
        <span class=n>encoder_layers</span><span class=o>=</span><span class=mi>24</span><span class=p>,</span>
    <span class=p>)</span>
    <span class=n>large_bert_model</span> <span class=o>=</span> <span class=n>Bert</span><span class=p>(</span><span class=n>large_bert_config</span><span class=p>,</span> <span class=n>padding_idx</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
    <span class=n>large_bert_param_count</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>large_bert_model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>
    <span class=c1># Make sure the parameter count matches the expected value for BERT large</span>
    <span class=c1># 335141888 match EXACTLY with Huggingface implementation: BertModel.from_pretrained("bert-large-uncased")</span>
    <span class=k>assert</span> <span class=n>large_bert_param_count</span> <span class=o>==</span> <span class=mi>335141888</span>  <span class=c1># 335M params</span>
</code></pre></div> <p>之后是训练过程的正确性，这个主要通过复现结果来验证，下面是我们的复现结果：</p> <table> <thead> <tr> <th style="text-align: center;">Metric</th> <th style="text-align: center;">Original BERT</th> <th style="text-align: center;">This Implementation</th> </tr> </thead> <tbody> <tr> <td style="text-align: center;">Perplexity(#L=12, #H=768, #A=12)</td> <td style="text-align: center;">3.99</td> <td style="text-align: center;">5.90</td> </tr> <tr> <td style="text-align: center;">SST2 Accuracy</td> <td style="text-align: center;">93.5%</td> <td style="text-align: center;">91.93%</td> </tr> </tbody> </table> <p>上面就是我们当前复现指标和论文给出指标的比较。 <strong>啊，是的，我们终究是没能完全复现论文给出的指标。</strong> 但其实我们因为算力限制，并没有做论文 1:1 的复现，差别如下所示：</p> <table> <thead> <tr> <th style="text-align: center;">Aspect</th> <th style="text-align: center;">Original BERT</th> <th style="text-align: center;">This Implementation</th> </tr> </thead> <tbody> <tr> <td style="text-align: center;">Max Sequence Length</td> <td style="text-align: center;">512</td> <td style="text-align: center;">128</td> </tr> <tr> <td style="text-align: center;">Pretraining Dataset</td> <td style="text-align: center;">BookCorpus + English Wikipedia</td> <td style="text-align: center;">BookCorpus only</td> </tr> <tr> <td style="text-align: center;">Training Epochs</td> <td style="text-align: center;">40</td> <td style="text-align: center;">23</td> </tr> </tbody> </table> <p>也就是说，我们用了更小的序列长度 (模型也略小一些)，更少的训练数据和更少的训练轮次。 虽然这些都是我们当前复现不及论文指标的原因，但个人认为训练数据缺失带来的影响是更大的。</p> <p>总而言之，这就是我们这里对“复现”的定义了 (代码完全开源在 <a href=https://github.com/ai-glimpse/toynlp/tree/master/toynlp/bert>ai-glimpse/toynlp</a>)。 虽然后续我可能会进行一次 1:1 的复现，但是目前为止，这就是我们复现的配置和得到的结果了。</p> <h2 id=bert>BERT 简介<a class=headerlink href=#bert title="Permanent link">¶</a></h2> <p>虽然我们这篇文章的重点是记录模型复现的过程和经验教训，并非模型本身。但是在这之前，我们还是简单来介绍一下 BERT 模型。</p> <p><img alt="BERT 论文中的模型架构" src=../../../../images/bert/bert_arch.png></p> <p>自 2017 年 Transformer(Encoder + Decoder 结构) 横空出世至今天，基于 Transformer 的模型架构存在多个不同发展分支，其中两个重要的分支就是 Encoder only 和 Decoder only：前者的代表就是 BERT，后者的代表就是 GPT(目前生成 LLM 的基石).</p> <p>如今大家对 GPT 都比较熟悉，其本质上是“文本续写”，即根据前面的文本生成后续的文字，比较像小学语文课里面的“故事续写”。GPT 训练的时候通过将文本的下一个词作为标签来训练模型根据上文 (<code>prompt</code>) 来生成下一个词的能力。</p> <p>BERT 的方法则更加类似于英语课中的“完形填空”，即根据待填空部分的上下文来选择当前位置的词语。 当然不同的是，我们自己做题只需要从 4 个候选项中选一个，而 BERT 模型需要从其高达 3 万 (30522) 的词表 (<code>vocab</code>) 中选出正确的答案。至于模型的训练，就是随机地将文本中的一些词语给掩盖掉 (打上<code>MASK</code>)，然后将其原来的词语作为标签进行预测。需要注意的是，BERT 对数据的预处理部分做得相当精细 (复杂)，这个可以具体参考实现代码来理解。</p> <p>在对 BERT 有个大致了解之后，下面就可以来聊一聊这长达一个多月的有趣的复现过程了。</p> <h2 id=_2>有趣的观察<a class=headerlink href=#_2 title="Permanent link">¶</a></h2> <h3 id=_3>预训练过程的有趣现象<a class=headerlink href=#_3 title="Permanent link">¶</a></h3> <p>我们在预训练阶段，复现了 <em>Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability</em> 这篇论文中观察到的预训练模型的学习模式。 注意这篇论文是基于 GPT 模型的研究，并非 BERT。但是我们在 BERT 预训练的时候也观察到类似的模式，即模型训练开始的时候会随机生成。 之后倾向于预测一些高频的 Token，比如 <code>the</code>, <code>.</code>等：</p> <div class=highlight><pre><span></span><code>Input Tokens: [CLS]|#|#|skyscr|##aper|arch|##ipe|##lag|##o|[SEP]|trying|to|recover|nuclear|weapons|now|[MASK]|[MASK]|bottom|[MASK]|the|ocean|.|.|.|.|[MASK]|were|trying|to|keep|a|nuclear|power|-|plant|from|melting|down|.|hait|.|.|[MASK]|high|[MASK]|ranking|officials|were|still|alive|at|the|submerged|un|compound|.|.|.|.|all|[MASK]|of|[SEP]
Target Tokens: skyscr|at|the|of|they|power|.|some|-|sorts
Predicted Tokens: .|.|.|.|.|.|.|.|.|.
</code></pre></div> <p>再之后才会出现一些真正有意义的预测：</p> <div class=highlight><pre><span></span><code>Input Tokens: [CLS]|"|never|mind|.|i|was|only|making|##ival|joke|familial|[MASK]|he|said|,|and|then|he|questioned|his|new|companion|.|"|i|don|'|t|feel|the|[MASK]|strap|anymore|.|how|is|it|that|i|delir|[MASK]|you|?|"|[SEP]|[MASK]|workers|be|sent|into|fields|for|the|harvest|.|the|very|next|verses|in|matthew|[MASK]|s|[MASK]|are|jesus|sending|out|the|apostles|to|do|the|exact|same|thing|he|had|been|[MASK]|(|matt|10|:|[MASK]|-|7|[MASK]|.|they|were|to|be|the|[MASK]|that|were|sent|out|to|gather|[MASK]|harvest|.|this|is|our|call|[MASK]|,|to|go|to|[MASK]|lost|,|hungry|and|thirsty|(|the|lost|sheep|of|this|[MASK]|[SEP]
Target Tokens: a|,|"|translator|anymore|can|understand|more|'|gospel|doing|matt|1|)|workers|the|too|the|age
[train]Predicted Tokens: a|,|"|same|anymore|can|about|more|'|gospel|born|matt|1|)|workers|the|too|the|age
</code></pre></div> <p><img alt src=../../../../images/bert/pretrain-pattern.png></p> <p>以下是我本人观察到这一现象后的一些自娱自乐：</p> <blockquote> <p>这其实和婴儿学习说话有些相似之处，比如刚出生不久大多都是哭（无意冒犯：哭声算是噪声，类似于上文提到的“随机生成”）；之后开始“咿咿呀呀”地重复说一些简单的字/词，如“这个”、“啊”等（类似上文的 <code>the</code>、<code>.</code>）；再之后就可以说一些有意义的词语和句子了。</p> </blockquote> <h3 id=_4>数据非常非常重要<a class=headerlink href=#_4 title="Permanent link">¶</a></h3> <p>在用全量的 BookCorpus 数据集进行预训练之前我们先进行了小规模数据 (10% 的样本) 训练以验证模型及训练脚本的正确性。下图是我们在小规模数据集和全量数据集上训练的结果对比：</p> <p><img alt src=../../../../images/bert/pretrain-data.png></p> <p>从图中可以看到： - 在前 5 个 epoch 中，全量数据集的困惑度远低于小规模数据集。例如，在第 5 个 epoch 时，全量数据集的测试困惑度约为 9.67，而小规模数据集的测试困惑度约为 70.41。 - 小规模数据集在 10 个 epoch 内就出现了过拟合，最佳测试困惑度约为 47.46。</p> <p>这也再次印证了数据对于预训练模型的重要性。虽然我们在小规模数据集上也能看到困惑度的下降，但整体表现远不及全量数据集。过拟合现象的出现也表明，有限的数据量限制了模型的泛化能力。</p> <h3 id=finetune>从 Finetune 管窥预训练的威力<a class=headerlink href=#finetune title="Permanent link">¶</a></h3> <p>如前文所述，我们采用 SST2 数据集对预训练好的 BERT 模型进行了微调。SST2 是一个二分类任务，旨在判断文本的情感倾向（正面或负面）。</p> <p>Finetune 模型的结构如下，就是一个简单的 BERT 模型加上一个线性分类器：</p> <p><img alt src=../../../../images/bert/bert-cls-finetune.png></p> <blockquote> <p>The figure is from <a href=https://web.stanford.edu/~jurafsky/slp3/ >Speech and Language Processing (3rd ed. draft)</a>.</p> </blockquote> <p>模型实现代码如下： </p><div class=highlight><pre><span></span><code><span class=k>class</span> <span class=nc>SST2BertModel</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>BertConfig</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>bert</span> <span class=o>=</span> <span class=n>Bert</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=n>padding_idx</span><span class=o>=</span><span class=n>bert_tokenizer</span><span class=o>.</span><span class=n>token_to_id</span><span class=p>(</span><span class=s2>"[PAD]"</span><span class=p>))</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>classifier</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>d_model</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>  <span class=c1># SST-2 has 2 classes</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>token_type_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
        <span class=n>bert_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bert</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>token_type_ids</span><span class=p>)</span>
        <span class=n>cls_hidden_state</span> <span class=o>=</span> <span class=n>bert_output</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>,</span> <span class=p>:]</span>
        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>classifier</span><span class=p>(</span><span class=n>cls_hidden_state</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>logits</span>
</code></pre></div> <p>我们首先看看没有预训练的 BERT 模型在 SST2 数据集上的表现。 注意我们这里使用了较小的<code>max_seq_length=128</code>(因为预训练时的输入长度就是 128)，而原始 BERT 使用的是<code>max_seq_length=512</code>。下面是在不同学习率下的训练曲线：</p> <p><img alt src=../../../../images/bert/SST2Bert-Without-Pretrain.png></p> <p>可以看到： - 使用学习率为<code>5e-5</code>时，模型在测试集上达到了约 80% 的最佳准确率。 - 使用学习率为<code>1e-5</code>和<code>1e-6</code>时，模型的性能更差。 - 所有学习率在经过若干个 epoch 后都出现了过拟合现象。</p> <p>接下来我们看看加载了预训练权重的 BERT 模型在 SST2 数据集上的表现：</p> <p><img alt src=../../../../images/bert/SST2Bert-With-Pretrain.png></p> <p>可以看到，在经过 1 个 epoch 的微调后，模型在测试集上的准确率就超过了 90%，更是在第 2 个 epoch 下达到了 91.93% 的准确率。</p> <p>另外在微调的时候也发现，经过少数几个 epoch 的训练后，模型就达到了最佳的效果，之后继续训练反而会导致过拟合。这可能就是论文中基本都微调 3 个 epoch 的原因：</p> <blockquote> <p>We use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks.</p> </blockquote> <p>通过对比可以明显看出预训练对模型性能的提升作用。预训练好的 BERT 模型在 SST2 数据集上表现出了更强的泛化能力和更快的收敛速度。</p> <h2 id=_5>错误与教训<a class=headerlink href=#_5 title="Permanent link">¶</a></h2> <p><img alt src=../../../../images/bert/times.png></p> <h3 id=_6>即使在极小数据集上也无法过拟合<a class=headerlink href=#_6 title="Permanent link">¶</a></h3> <p>我想先确认从数据加载、分词到模型训练的整条流水线都是正确的，所以在放大规模之前，我想先确保模型能在一个很小的数据集上过拟合。为了加快“刻意过拟合”的速度，我把学习率调大（0.01/0.001）。但我发现，即便在非常小的数据集上，也过拟合不起来。</p> <p>我花了很多时间，尝试了各种方法来排查问题： - 重新检查数据准备流程 - 校验模型结构 - 尝试不同的超参数 - 确保 mask token 的使用是正确的 - ……</p> <p>这些尝试都没能让模型在小数据集上过拟合。你可能会问：大模型/AI 能帮上忙吗？答案是不行！真的不行。它们给了很多没有意义的建议。于是我换个思路，用更小的数据集 (只用一个样本训练)，并在代码里多加打印，直观地看看问题出在哪。 最终，我看到了这样的输出：</p> <div class=highlight><pre><span></span><code>====================================================================================================
Epoch 146/1000 - Train Loss: 2.3028, Train MLM Loss: 2.3028, Train NSP Loss: 0.0000, Train NSP Accuracy: 1.0000,
====================================================================================================
Input Tokens: [CLS]|#|#|skyscr|##aper|arch|##ipe|##lag|##o|[SEP]|trying|to|recover|nuclear|weapons|now|[MASK]|[MASK]|bottom|[MASK]|the|ocean|.|.|.|.|[MASK]|were|trying|to|keep|a|nuclear|power|-|plant|from|melting|down|.|hait|.|.|[MASK]|high|[MASK]|ranking|officials|were|still|alive|at|the|submerged|un|compound|.|.|.|.|all|[MASK]|of|[SEP]
Target Tokens: skyscr|at|the|of|they|power|.|some|-|sorts
Predicted Tokens: at|at|at|at|at|at|at|at|at|at
====================================================================================================
Epoch 147/1000 - Train Loss: 2.3028, Train MLM Loss: 2.3028, Train NSP Loss: 0.0000, Train NSP Accuracy: 1.0000,
====================================================================================================
Input Tokens: [CLS]|#|#|skyscr|##aper|arch|##ipe|##lag|##o|[SEP]|trying|to|recover|nuclear|weapons|now|[MASK]|[MASK]|bottom|[MASK]|the|ocean|.|.|.|.|[MASK]|were|trying|to|keep|a|nuclear|power|-|plant|from|melting|down|.|hait|.|.|[MASK]|high|[MASK]|ranking|officials|were|still|alive|at|the|submerged|un|compound|.|.|.|.|all|[MASK]|of|[SEP]
Target Tokens: skyscr|at|the|of|they|power|.|some|-|sorts
Predicted Tokens: some|some|some|some|some|some|some|some|some|some
====================================================================================================
</code></pre></div> <p>损失一直在 2.3 附近上下抖动（主要是 MLM 损失，NSP 损失几乎为 0）。 这让我意识到模型可能卡在了一个局部极小点，而原因很可能是<strong>学习率过大</strong>。 于是我把学习率调小到 <code>0.0001</code>，训练损失就能下降到 0.0，预测的 token 也终于能与目标对齐：</p> <div class=highlight><pre><span></span><code>====================================================================================================
Epoch 29/1000 - Train Loss: 0.0004, Train MLM Loss: 0.0000, Train NSP Loss: 0.0004, Train NSP Accuracy: 1.0000,
====================================================================================================
Input Tokens: [CLS]|#|#|skyscr|##aper|arch|##ipe|##lag|##o|[SEP]|trying|to|recover|nuclear|weapons|now|[MASK]|[MASK]|bottom|[MASK]|the|ocean|.|.|.|.|[MASK]|were|trying|to|keep|a|nuclear|power|-|plant|from|melting|down|.|reap|.|.|[MASK]|high|[MASK]|ranking|officials|were|still|alive|at|the|submerged|un|compound|.|.|.|.|all|[MASK]|of|[SEP]
Target Tokens: skyscr|at|the|of|they|power|.|some|-|sorts
Predicted Tokens: skyscr|at|the|of|they|power|.|some|-|sorts
====================================================================================================
Epoch 30/1000 - Train Loss: 0.0000, Train MLM Loss: 0.0000, Train NSP Loss: 0.0000, Train NSP Accuracy: 1.0000,
====================================================================================================
Input Tokens: [CLS]|#|#|skyscr|##aper|arch|##ipe|##lag|##o|[SEP]|trying|to|recover|nuclear|weapons|now|[MASK]|[MASK]|bottom|[MASK]|the|ocean|.|.|.|.|[MASK]|were|trying|to|keep|a|nuclear|power|-|plant|from|melting|down|.|reap|.|.|[MASK]|high|[MASK]|ranking|officials|were|still|alive|at|the|submerged|un|compound|.|.|.|.|all|[MASK]|of|[SEP]
Target Tokens: skyscr|at|the|of|they|power|.|some|-|sorts
Predicted Tokens: skyscr|at|the|of|they|power|.|some|-|sorts
====================================================================================================
</code></pre></div> <blockquote> <p>为了加快过拟合速度，我把学习率设得更大（0.01/0.001）。</p> </blockquote> <p>故事大概就是这样：更大的学习率确实能让模型更快收敛，但也更容易把你带进局部极小值里。(嗯，很老套的错误...)</p> <p>至于我为什么这么着急想让模型更快训练完？因为我的 4060Ti 实在太慢了，只想尽快结束训练。 总之，还是要耐心点，慢就是快。</p> <h3 id=cpu-gpu>一次性预处理全量数据，CPU 加班，GPU 休假<a class=headerlink href=#cpu-gpu title="Permanent link">¶</a></h3> <p>在开始的实现中，我把数据集的转换放在了数据加载环节，一次性把整个数据集都转换完：</p> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>get_split_dataloader</span><span class=p>(</span>
    <span class=n>dataset_path</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
    <span class=n>split</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
    <span class=n>config</span><span class=p>:</span> <span class=n>BertConfig</span><span class=p>,</span>
<span class=p>)</span> <span class=o>-&gt;</span> <span class=n>DataLoader</span><span class=p>:</span>
    <span class=n>raw_dataset</span> <span class=o>=</span> <span class=n>get_dataset</span><span class=p>(</span><span class=n>dataset_path</span><span class=p>,</span> <span class=kc>None</span><span class=p>,</span> <span class=n>split</span><span class=p>)</span>  <span class=c1># type: ignore[call-arg]</span>
    <span class=n>pretrain_dataset</span> <span class=o>=</span> <span class=n>dataset_transform</span><span class=p>(</span><span class=n>raw_dataset</span><span class=p>,</span> <span class=n>config</span><span class=p>)</span>
    <span class=n>dataloader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span>
        <span class=n>pretrain_dataset</span><span class=o>.</span><span class=n>with_format</span><span class=p>(</span><span class=nb>type</span><span class=o>=</span><span class=s2>"torch"</span><span class=p>),</span>
        <span class=n>batch_size</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>batch_size</span><span class=p>,</span>
        <span class=n>collate_fn</span><span class=o>=</span><span class=k>lambda</span> <span class=n>batch</span><span class=p>:</span> <span class=n>collate_fn</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>bert_tokenizer</span><span class=p>),</span>
    <span class=p>)</span>
</code></pre></div> <p>这行 <code>pretrain_dataset = dataset_transform(raw_dataset, config)</code> 会一次性把整个数据集转换完，这是非常耗时的操作（12 颗 2.1GHz CPU 也花了 7 天多……）。 更好的做法是把这步放到训练时按 batch 即时转换。(但这并没有想象中那么简单，见下文)</p> <h3 id=_7>试图用临时手搓的玩意儿达到很好的性能<a class=headerlink href=#_7 title="Permanent link">¶</a></h3> <p>为了解决上面的问题，我尝试自己做一个缓存 (Producer-&gt;Cache&lt;-Consumer) 来存放已转换好的样本，但效率依然很低。（快速搓的一个很挫的版本，性能差也是预期内的）</p> <p>后来我意识到，与其自己造一个看似简单（其实并不简单）的缓存，不如再看看能不能用 <code>datasets</code> 库和 <code>DataLoader</code> 自带的能力来优化数据加载与转换。我相信自己遇到的问题并不罕见，社区里应该有人解决过类似的问题。</p> <p>然后就发现 <code>datasets</code> 有 <code>streaming=True</code> 的配置，正是这里需要的流式加载功能：</p> <div class=highlight><pre><span></span><code><span class=n>dataset</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=n>path</span><span class=o>=</span><span class=n>dataset_path</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=n>dataset_name</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=n>split</span><span class=p>,</span> <span class=n>streaming</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=o>...</span>
<span class=n>raw_dataset</span> <span class=o>=</span> <span class=n>raw_dataset</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span><span class=o>.</span><span class=n>to_iterable_dataset</span><span class=p>(</span><span class=n>num_shards</span><span class=o>=</span><span class=mi>32</span><span class=p>)</span>
</code></pre></div> <p>在 DataLoader 中使用 <code>prefetch_factor</code> 提升数据加载吞吐：</p> <div class=highlight><pre><span></span><code><span class=n>dataloader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span>
    <span class=n>pretrain_dataset</span><span class=o>.</span><span class=n>with_format</span><span class=p>(</span><span class=nb>type</span><span class=o>=</span><span class=s2>"torch"</span><span class=p>),</span>
    <span class=n>batch_size</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>batch_size</span><span class=p>,</span>
    <span class=n>collate_fn</span><span class=o>=</span><span class=k>lambda</span> <span class=n>batch</span><span class=p>:</span> <span class=n>collate_fn</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>bert_tokenizer</span><span class=p>),</span>
    <span class=n>num_workers</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
    <span class=n>prefetch_factor</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
    <span class=n>pin_memory</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>persistent_workers</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
<span class=p>)</span>
</code></pre></div> <p>这个方法基本解决了“GPU 等 CPU”的问题，GPU 利用率也基本可以处于打满的状态。 但它又间接引起了其他问题，见下文。</p> <h3 id=workers-prefetch-oom>workers 和 prefetch 因子设得过大导致 OOM<a class=headerlink href=#workers-prefetch-oom title="Permanent link">¶</a></h3> <p>当我设置 <code>num_workers=16</code>（我的 CPU 有 16 核）和 <code>prefetch_factor=10</code> 时，GPU 的确不再闲着了，但 DataLoader 进程的内存一直上涨，最终把系统内存（16GB）吃光，被 OOM 杀掉 (大约训练 16 小时后触发)：</p> <div class=highlight><pre><span></span><code><span class=n>File</span> <span class=s2>"/toynlp/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py"</span><span class=p>,</span> <span class=n>line</span> <span class=mi>125</span><span class=p>,</span> <span class=ow>in</span> <span class=n>forward</span>
    <span class=k>return</span> <span class=n>F</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=nb>input</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias</span><span class=p>)</span>
           <span class=o>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span>
<span class=n>File</span> <span class=s2>"/toynlp/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py"</span><span class=p>,</span> <span class=n>line</span> <span class=mi>73</span><span class=p>,</span> <span class=ow>in</span> <span class=n>handler</span>
    <span class=n>_error_if_any_worker_fails</span><span class=p>()</span>
<span class=ne>RuntimeError</span><span class=p>:</span> <span class=n>DataLoader</span> <span class=n>worker</span> <span class=p>(</span><span class=n>pid</span> <span class=mi>2972</span><span class=p>)</span> <span class=ow>is</span> <span class=n>killed</span> <span class=n>by</span> <span class=n>signal</span><span class=p>:</span> <span class=n>Killed</span><span class=o>.</span>
</code></pre></div> <p>我通过减小 <code>num_workers</code> 和 <code>prefetch_factor</code>，并去掉 <code>pin_memory</code> 与 <code>persistent_workers</code> 选项来缓解：</p> <div class=highlight><pre><span></span><code><span class=n>dataloader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span>
    <span class=n>pretrain_dataset</span><span class=o>.</span><span class=n>with_format</span><span class=p>(</span><span class=nb>type</span><span class=o>=</span><span class=s2>"torch"</span><span class=p>),</span>
    <span class=n>batch_size</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>batch_size</span><span class=p>,</span>
    <span class=n>collate_fn</span><span class=o>=</span><span class=k>lambda</span> <span class=n>batch</span><span class=p>:</span> <span class=n>collate_fn</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>bert_tokenizer</span><span class=p>),</span>
    <span class=n>num_workers</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span>
    <span class=n>prefetch_factor</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
<span class=p>)</span>
</code></pre></div> <p>根据 <a href=https://github.com/pytorch/pytorch/issues/13246>pytorch/issues/13246</a> 和 <a href=https://github.com/huggingface/datasets/issues/7269>datasets/issues/7269</a> 上关于内存泄漏的讨论，为了尽量避免 OOM，我参考上面的讨论在每个 epoch 后调用 <code>torch.cuda.empty_cache()</code> 来清理显存。这些变更应用之后，长时间的训练终于没有再出现任何 OOM 问题。</p> <h3 id=dataloader-worker>没有优雅处理 DataLoader worker 的异常<a class=headerlink href=#dataloader-worker title="Permanent link">¶</a></h3> <p>在用 <strong>10%</strong> BookCorpus 数据训练时一切正常。 但当我切到 <strong>全量</strong> BookCorpus 数据集后，训练约 26 小时发生了异常，直接导致训练流程中断：</p> <div class=highlight><pre><span></span><code>  <span class=n>File</span> <span class=s2>"/app/toynlp/toynlp/bert/dataset.py"</span><span class=p>,</span> <span class=n>line</span> <span class=mi>344</span><span class=p>,</span> <span class=ow>in</span> <span class=o>&lt;</span><span class=k>lambda</span><span class=o>&gt;</span>
    <span class=n>batch_instances</span> <span class=o>:=</span> <span class=n>batch_create_pretraining_examples_from_documents</span><span class=p>(</span>
                       <span class=o>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span>
  <span class=n>File</span> <span class=s2>"/app/toynlp/toynlp/bert/dataset.py"</span><span class=p>,</span> <span class=n>line</span> <span class=mi>301</span><span class=p>,</span> <span class=ow>in</span> <span class=n>batch_create_pretraining_examples_from_documents</span>
    <span class=n>random_document</span> <span class=o>=</span> <span class=n>batch</span><span class=p>[</span><span class=n>rng</span><span class=o>.</span><span class=n>choice</span><span class=p>([</span><span class=n>j</span> <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>batch</span><span class=p>))</span> <span class=k>if</span> <span class=n>j</span> <span class=o>!=</span> <span class=n>i</span><span class=p>])]</span>
                            <span class=o>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span>
  <span class=n>File</span> <span class=s2>"/root/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/random.py"</span><span class=p>,</span> <span class=n>line</span> <span class=mi>347</span><span class=p>,</span> <span class=ow>in</span> <span class=n>choice</span>
    <span class=k>raise</span> <span class=ne>IndexError</span><span class=p>(</span><span class=s1>'Cannot choose from an empty sequence'</span><span class=p>)</span>
<span class=ne>IndexError</span><span class=p>:</span> <span class=n>Cannot</span> <span class=n>choose</span> <span class=kn>from</span> <span class=nn>an</span> <span class=n>empty</span> <span class=n>sequence</span>
</code></pre></div> <p>于是我在 DataLoader worker 里加了不少 try-except，把这类错误尽量“吞掉并跳过”，让训练能够在数据异常时继续跑。你可以在当前实现（<code>bert/dataset.py</code>）里看到这些处理。 代码的实现目前不够优雅，但确实解决了问题。</p> <h3 id=tokenizer>评估时使用了错误的 tokenizer<a class=headerlink href=#tokenizer title="Permanent link">¶</a></h3> <p>在用 SST2 做微调评估预训练模型时，我一度错用了旧的 tokenizer（基于 BookCorpus 10% 训练） 而不是新的 tokenizer（基于 90% 训练，留出的 10% 用于验证/测试）来分词，导致微调后的 SST2 表现明显变差。 用错 tokenizer 时最佳测试准确率约 83%，而用正确的 tokenizer 后，最佳测试准确率约 89%。(这是预训练中间态模型的结果，最终模型表现更好，见上文结果表格。)</p> <h2 id=_8>最后<a class=headerlink href=#_8 title="Permanent link">¶</a></h2> <p>在这长达一个多月的复现过程中，我踩了很多的坑，也学到了很多经验。这些宝贵的经验教训让我对预训练有了更深的理解——这是从读论文中所无法获得的。 在踩了这些坑之后，我隐隐约约地看到科学先辈的筚路蓝缕；在这段旅程结束时，我觉得自己也找到了一块算不得好看的鹅卵石，而“真理的大海则横陈在我面前”。 最后，希望每一位读者都能踏上属于自己的奇妙旅程。</p></div> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title=最后更新> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">2025年11月2日</span> </span> <span class=md-source-file__fact> <span class=md-icon title=创建日期> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">2025年11月2日</span> </span> <span class=md-source-file__fact> <span class=md-icon title=贡献者> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34-.46-1.16-1.11-1.47-1.11-1.47-.91-.62.07-.6.07-.6 1 .07 1.53 1.03 1.53 1.03.87 1.52 2.34 1.07 2.91.83.09-.65.35-1.09.63-1.34-2.22-.25-4.55-1.11-4.55-4.92 0-1.11.38-2 1.03-2.71-.1-.25-.45-1.29.1-2.64 0 0 .84-.27 2.75 1.02.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02.55 1.35.2 2.39.1 2.64.65.71 1.03 1.6 1.03 2.71 0 3.82-2.34 4.66-4.57 4.91.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"/></svg> </span> <span>GitHub</span> <nav> <a href=https://github.com/shenxiangzhuang class=md-author title=@shenxiangzhuang> <img src="https://avatars.githubusercontent.com/u/17157965?v=4&size=72" alt=shenxiangzhuang> </a> <a href=https://github.com/web-flow class=md-author title=@web-flow> <img src="https://avatars.githubusercontent.com/u/19864447?v=4&size=72" alt=web-flow> </a> </nav> </span> </aside> <script src=https://giscus.app/client.js data-repo=shenxiangzhuang/shenxiangzhuang.github.io data-repo-id="MDEwOlJlcG9zaXRvcnk4MjU0MjM1OQ==" data-category=Announcements data-category-id=DIC_kwDOBOt_F84CTHBH data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async>
</script> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> 回到页面顶部 </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2017 - 2024 Xiangzhuang Shen </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://datahonor.com/feed_rss_created target=_blank rel=noopener title=datahonor.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M6.18 15.64a2.18 2.18 0 0 1 2.18 2.18C8.36 19 7.38 20 6.18 20 5 20 4 19 4 17.82a2.18 2.18 0 0 1 2.18-2.18M4 4.44A15.56 15.56 0 0 1 19.56 20h-2.83A12.73 12.73 0 0 0 4 7.27zm0 5.66a9.9 9.9 0 0 1 9.9 9.9h-2.83A7.07 7.07 0 0 0 4 12.93z"/></svg> </a> <a href=https://github.com/shenxiangzhuang target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://twitter.com/MathewShen42 target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9L389.2 48zm-24.8 373.8h39.1L151.1 88h-42l255.3 333.8z"/></svg> </a> <a href=https://linkedin.com/in/mathewshen target=_blank rel=noopener title=linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> <a href=mailto:datahonor@gmail.com target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2"/></svg> </a> <a href=https://mp.weixin.qq.com/s/wOqp6nHBAenK9wP2vIUS9g target=_blank rel=noopener title=mp.weixin.qq.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.691 2.188C3.891 2.188 0 5.476 0 9.53c0 2.212 1.17 4.203 3.002 5.55a.59.59 0 0 1 .213.665l-.39 1.48c-.019.07-.048.141-.048.213 0 .163.13.295.29.295a.33.33 0 0 0 .167-.054l1.903-1.114a.86.86 0 0 1 .717-.098 10.2 10.2 0 0 0 2.837.403c.276 0 .543-.027.811-.05-.857-2.578.157-4.972 1.932-6.446 1.703-1.415 3.882-1.98 5.853-1.838-.576-3.583-4.196-6.348-8.596-6.348M5.785 5.991c.642 0 1.162.529 1.162 1.18a1.17 1.17 0 0 1-1.162 1.178A1.17 1.17 0 0 1 4.623 7.17c0-.651.52-1.18 1.162-1.18zm5.813 0c.642 0 1.162.529 1.162 1.18a1.17 1.17 0 0 1-1.162 1.178 1.17 1.17 0 0 1-1.162-1.178c0-.651.52-1.18 1.162-1.18m5.34 2.867c-1.797-.052-3.746.512-5.28 1.786-1.72 1.428-2.687 3.72-1.78 6.22.942 2.453 3.666 4.229 6.884 4.229.826 0 1.622-.12 2.361-.336a.72.72 0 0 1 .598.082l1.584.926a.3.3 0 0 0 .14.047c.134 0 .24-.111.24-.247 0-.06-.023-.12-.038-.177l-.327-1.233a.6.6 0 0 1-.023-.156.49.49 0 0 1 .201-.398C23.024 18.48 24 16.82 24 14.98c0-3.21-2.931-5.837-6.656-6.088V8.89c-.135-.01-.27-.027-.407-.03zm-2.53 3.274c.535 0 .969.44.969.982a.976.976 0 0 1-.969.983.976.976 0 0 1-.969-.983c0-.542.434-.982.97-.982zm4.844 0c.535 0 .969.44.969.982a.976.976 0 0 1-.969.983.976.976 0 0 1-.969-.983c0-.542.434-.982.969-.982"/></svg> </a> <a href=https://www.zhihu.com/people/shen-xiang-zhuang target=_blank rel=noopener title=www.zhihu.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M5.721 0C2.251 0 0 2.25 0 5.719V18.28C0 21.751 2.252 24 5.721 24h12.56C21.751 24 24 21.75 24 18.281V5.72C24 2.249 21.75 0 18.281 0zm1.964 4.078q-.408 1.096-.68 2.11h4.587c.545-.006.445 1.168.445 1.171H9.384a58 58 0 0 1-.112 3.797h2.712c.388.023.393 1.251.393 1.266H9.183a9.2 9.2 0 0 1-.408 2.102l.757-.604c.452.456 1.512 1.712 1.906 2.177.473.681.063 2.081.063 2.081l-2.794-3.382c-.653 2.518-1.845 3.607-1.845 3.607-.523.468-1.58.82-2.64.516 2.218-1.73 3.44-3.917 3.667-6.497H4.491c0-.015.197-1.243.806-1.266h2.71c.024-.32.086-3.254.086-3.797H6.598c-.136.406-.158.447-.268.753-.594 1.095-1.603 1.122-1.907 1.155.906-1.821 1.416-3.6 1.591-4.064.425-1.124 1.671-1.125 1.671-1.125M13.078 6h6.377v11.33h-2.573l-2.184 1.373-.401-1.373h-1.219zm1.313 1.219v8.86h.623l.263.937 1.455-.938h1.456v-8.86z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-consent data-md-component=consent id=__consent hidden> <div class=md-consent__overlay></div> <aside class=md-consent__inner> <form class="md-consent__form md-grid md-typeset" name=consent> <h4>Cookie consent</h4> <p>We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better.</p> <input class=md-toggle type=checkbox id=__settings> <div class=md-consent__settings> <ul class=task-list> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=analytics checked> <span class=task-list-indicator></span> Google Analytics </label> </li> <li class=task-list-item> <label class=task-list-control> <input type=checkbox name=github checked> <span class=task-list-indicator></span> GitHub </label> </li> </ul> </div> <div class=md-consent__controls> <button class="md-button md-button--primary">同意</button> <label class=md-button for=__settings>管理设定</label> </div> </form> </aside> </div> <script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script> <script id=__config type=application/json>{"base": "../../../../..", "features": ["content.action.edit", "content.action.view", "announce.dismiss", "content.code.annotate", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.indexes", "navigation.prune", "navigation.sections", "navigation.top", "search.highlight", "search.share", "search.suggest", "toc.follow", "content.code.annotate"], "search": "../../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script> <script src=../../../../../assets/javascripts/bundle.83f73b43.min.js></script> <script src=../../../../../javascripts/mathjax.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>